# Distributed Systems(分布式系统)
 
## 概念
```shell
  1. 服务注册
        指一个服务将位置信息向“中心注册节点”注册。该服务一般会将它的主机 IP 地址以及端口号进行注册，有时也会有服务访问的认证信息，
        使用协议，版本号，以及关于环境的一些细节信息
  2. 服务发现
        服务发现可以让一个应用(或者组件)发现其运行环境以及其它应用或组件的信息。用户配置一个服务发现工具就可以将实际容器跟
        运行配置分离。 常见配置信息包括：ip、端口号、名称等
        
        服务发现是为了解决传统服务注册的的缺点(当服务存在于多个主机节点上时，都会使用静态配置的方法来实现服务信息的注册), 
        这种方式其扩展性很差，需要静态的配置服务信息(静态服务注册)，静态配置的方法一般是写在配置文件或则数据库中。
        客户端通过配置文件(或则数据库) 拿到 ip1:port1 向服务 A 请求消息， 通过配置文件(或则数据库) 拿到 ip2:port2 
        向服务 B 请求消息.
        
        服务发现的做法：在客户端和服务之间加一个服务发现模块， 服务A-N把当前自己的网络位置注册到服务发现模块，
        服务发现就以 K-V 的方式记录下， K一般是服务名，V 就是IP:PORT。服务发现模块定时的轮询查看这些服务能不能访问（健康检查）。
        客户端在调用服务A-N的时候，就去服务发现模块问下它们的网络位置， 然后再调用它们的服务。
        
        服务发现的框架有 zookeeper, eureka, etcd, consul
        
```
 
## 分布式起源
```shell
    1. 单机模式
            (1) 概念: 所有应用程序和数据均部署同一台服务器上.
            (2) 优点: 功能、代码和数据集中，便于维护、管理和执行
            (2) 缺点: 性能受限、存在单点失效问题
            
    2. 数据并行或数据分布式
            (1) 概念: 每台服务都运行相同的程序, 并且数据拆分到不同的服务器上, 并行的计算.
            (2) 前提: 第一: 应用和数据要进行拆分, 其拆分到不同服务器上.　第二： 数据与数据也要进行拆分(这个数据拆分方式按照业务来)
            (3) 问题
                    问题一: 负载均衡问题, 怎么将大量的请求均衡分布到各个处理服务器上
                    问题二: IO 性能问题, 当请求较大时, 频繁读写数据库, 使得数据库 IO 性能存在问题, 解决方案是进行读写分离, 
                           读写数据库之间要进行数据同步,以保证数据一致性
                    问题三: 有些数据频繁访问, 会导致数据库中的热点数据被频繁访问. 解决方案是引入缓存机制,减轻数据库压力,同时提高查询效率
            (4) 对提升单个任务的执行性能及降低时延无效
    3. 任务并行或任务分布式
            (1) 概念: 将单个复杂的任务拆分为多个子任务,使多个子任务可以在不同的服务器上并行执行
            (2) 提高性能、扩展性、可维护性等的同时, 也带来了设计上的复杂性问题，比如复杂任务的拆分
    4. 数据并行、任务并行其实都可以算作是分布式的一种形态
    5. 选取的原则: 
                任务执行时间短，数据规模大、类型相同且无依赖，则可采用数据并行
                如果任务复杂、执行时间长，且任务可拆分为多个子任务，则考虑任务并行
                    
```
 
## 分布式系统的指标
```shell
    1. 性能(Performance)
            (1) 吞吐量(Throughput): 吞吐量（Throughput）
                    a. QPS（Queries Per Second）
                            衡量一个系统每秒的查询数. 这个指标通常用于读操作，越高说明对读操作的支持越好.
                    b. TPS（Transactions Per Second）
                            衡量一个系统每秒处理的事务数, 这个指标通常对应于写操作，越高说明对写操作的支持越好. 
                       在设计一个分布式系统的时候，如果应用主要是写操作，需要重点考虑如何提高 TPS, 来支持高频写操作
                    c. BPS（Bits Per Second）
                            衡量一个系统每秒处理的数据量, 对于一些网络系统、数据管理系统, 不能简单地按照请求数或事务数来衡量其性能.
                       因为请求与请求、事务与事务之间也存在着很大的差异，比方说，有的事务大需要写入更多的数据.那么在这种情况下,
                       BPS 更能客观地反应系统的吞吐量。
                       
            (2) 响应时间（Response Time）
                    系统从输入到获得结果的时间.
            (3) 完成时间（Turnaround Time）
                    系统真正完成一个请求或处理需要花费的时间
                    
    2. 资源占用（Resource Usage）
            (1) 指占用的硬件资源, 比如 CPU、内存、硬盘等
            (2) 空载资源占用: 在没有任何负载的资源占用.
                满载资源占用
                
    3. 可用性（Availability）
            (1) 系统正常使用的情况 / 系统所有的次数, 例如 系统正常服务的时间与总的时间之比，　某功能的正常次数与总的请求次数之比
            (2) 可靠性用来表示一个系统完全不出故障的概率, 更多地用在硬件领域。 
                可用性则是指在允许部分组件失效的情况下，一个系统对外仍能正常提供服务的概率
                
    4. 可扩展性（Scalability）
            (1) 通过增加机器的方式去水平/横向扩展系统规模
            (2) 衡量可扩展性指标是 加速比（Speedup）, 指的是一个系统进行扩展后相对扩展前的性能提升
```
 
## 分布式协调与同步
 
### 分布式协调与同步 --- 分布式互斥
```shell
        (1) 集中式算法: 有一个协调程序, 专门统一管理临界资源, 其他程序想要使用临界资源, 需要向协调程序申请, 如果临界资源
        　　　　　　　　　空闲, 则协调程序通知程序1, 如果临界资源被占用, 则需要在协调程序中排队.
                 优点: 直观、简单、信息交互量少、易于实现，并且所有程序只需和协调者通信，程序之间无需通信
                 缺点: 
                        第一, 协调者会成为系统的性能瓶颈, 协调者需要处理程序的请求(程序的申请, 释放, 向程序的发放)会随着需要
                              访问临界资源的程序数量线性增加.
                        第二, 容易引发单点故障, 如果协调者出现异常, 则导致所有程序无法访问临界资源, 系统不可用.
                 总结；
                        集中式算法具有简单、易于实现的特点， 但可用性、性能易受协调者影响. 在可靠性和性能有一定保障的情况下,
                        比如中央服务器计算能力强、性能高、故障率低，或者中央服务器进行了主备备份，主故障后备可以立马升为主, 
                        且数据可恢复的情况下, 集中式算法可以适用于比较广泛的应用场景
                        
                 改进:
                        集中式算法可以考虑把协调节点做成集群, 参与者通过资源做一致性 hash 找对应的协调节点,
                        这样可以一定程度解决单点问题
                        
        (2) 分布式算法: 当一个程序要访问临界资源时, 先向系统中的其他程序发送一条请求消息, 在接收到所有程序返回的同意消息后,
                      才可以访问临界资源. 请求消息需要包含所请求的资源、请求者的 ID，以及发起请求的时间
                      
                      过程:
                            一个程序完成一次临界资源的访问，需要进行如下的信息交互: 
                                第一: 向其他 n-1 个程序发送访问临界资源的请求，总共需要 n-1 次消息交互；
                                第二: 需要接收到其他 n-1 个程序回复的同意消息，方可访问资源，总共需要 n-1 次消息交互
                      问题:
                            1. 当系统内需要访问临界资源的程序增多时，容易产生“信令风暴”，也就是程序收到的请求完全超过了自己的
                               处理能力，而导致自己正常的业务无法开展
                            2. 一旦某一程序发生故障，无法发送同意消息，那么其他程序均处在等待回复的状态中,
                               使得整个系统处于停滞状态, 导致整个系统不可用。所以，相对于集中式算法的协调者故障,
                               分布式算法的可用性更低
                               
                      改进:
                            1. 每个程序需要对其他程序进行故障检测, 请求临界资源的访问, 只向正常通信的程序进行请求.
                            2. 采用大多数投票通过, 而不是全部通过
                      适用场景:
                            适合节点数目少且变动不频繁的系统(临界资源使用频度较低，且系统规模较小),
                            且由于每个程序均需通信交互, 例如 P2P 结构的系统
                            
        (3) 令牌环算法: 所有程序构成一个环结构，令牌按照顺时针（或逆时针）方向在程序之间传递，收到令牌的程序有权访问临界资源，
                      访问完成后将令牌传送到下一个程序；若该程序不需要访问临界资源,则直接把令牌传送给下一个程序
                      
                      问题:
                            在一个周期内每个程序都会收到令牌(不管这个程序需不需要), 这样可能存在只有一个程序需要临界资源, 其他
                            的都不需要, 而这个程序可能位置不好, 再后面才拿到, 实时性弱.
                            
                      优点:
                            很好地解决单点故障问题, 提高系统的健壮性，带来更好的可用性, 不过得要求每个程序都要记住环中的参与者信息,
                            这样才能知道在跳过一个参与者后令牌应该传递给谁
                            
                      总结:
                            适用于系统规模较小，并且系统中每个程序使用临界资源的频率高且使用时间比较短的场景
                            
                      改进:
                            增加轮值权重
                            
        (4) 适用大规模的系统　两层结构的分布式令牌环算法
                    原理: 
                        广域网由多个局域网组, 局域网是较低的层次，广域网是较高的层次, 每个局域网中包含若干个局部进程和一个协调进程
                        局部进程在逻辑上组成一个环形结构，在每个环形结构上有一个局部令牌 T 在局部进程间传递, 局域网与局域网之间通过
                        各自的协调进程进行通信，这些协调进程同样组成一个环结构，这个环就是广域网中的全局环
                         
```
 
### 分布式协调与同步 --- 分布式选举
```shell
    1. 概念
           分布式选举是为了选出主节点, 该主节点在一个分布式集群中负责对其他节点的协调和管理, 其保证了其他节点的有序运行, 
           以及数据库集群中的写入数据在每个节点上的一致性
           
           分布式选举问题，其实就是传统的分布式共识方法，主要是基于多数投票策略实现的
    2. 选举算法
            (1) Bully 算法
                    概念: 在所有在线的节点中，选取 ID 最大的节点作为主节点
                    过程：
                            Bully 算法在选举过程, 有 3 中消息, 第一种 Election 消息, 用于发起选举. 第二种 Alive 消息,
                         对 Election 消息的应答. 第三种 Victory 消息，竞选成功的主节点向其他节点发送的宣誓主权的消息.
                         前提要求是集群中每个节点都知道其他节点的　ID
                         
                         1. 集群中每个节点判断自己的 ID 是否为当前活着的节点中 ID 最大的, 如果是, 则直接向其他节点发送 Victory 消息,
                            宣誓自己的主权；
                         2. 如果自己不是当前活着的节点中 ID 最大的，则向比自己 ID 大的所有节点发送 Election 消息，并等待其他节点的回复；
                         3. 若在给定的时间范围内，本节点没有收到其他节点回复的 Alive 消息，则认为自己成为主节点，
                            并向其他节点发送 Victory 消息，宣誓自己成为主节点；若接收到来自比自己 ID 大的节点的 Alive 消息，
                            则等待其他节点发送 Victory 消息；
                         4. 若本节点收到比自己 ID 小的节点发送的 Election 消息，则回复一个 Alive 消息，告知其他节点，我比你大,
                            重新选举
                            
                    优点:
                        选举速度快、算法复杂度低、简单易实现
                    缺点:
                        需要每个节点都要保存全局的节点信息,其额外的存储信息多, 同时任意一个比当前主节点 ID 大的新节点或则节点故障后
                        恢复加入集群的时候, 都可能会触发重新选举，成为新的主节点. 如果该节点频繁退出、加入集群，就会导致频繁切主.
                        
            (2) Raft 算法
                    概念: 多数派投票选举算法, 获得投票最多的节点成为主
                    角色分类:
                            Leader: 即主节点，同一时刻只有一个 Leader，负责协调和管理其他节点
                            Candidate: 即候选者，每一个节点都可以成为 Candidate，节点在该角色下才可以被选为新的 Leader
                            Follower: Leader 的跟随者，不可以发起选举
                    过程:
                            第一步: 初始化时，所有节点均为 Follower 状态。
                            第二步: 开始选主时，所有节点的状态由 Follower 转化为 Candidate，并向其他节点发送选举请求。
                            第三步: 其他节点根据接收到的选举请求的先后顺序，回复是否同意成为主。在每一轮选举中，一个节点只能投出一张票。
                            第四步: 若发起选举请求的节点获得超过一半的投票，则成为主节点，其状态转化为 Leader，
                                   其他节点的状态则由 Candidate 降为 Follower. Leader 节点与 Follower 节点之间会定期发送心跳包,
                                   以检测主节点是否活着。
                            第五步；当 Leader 节点的任期到了，即发现其他服务器开始下一轮选主周期时, Leader 节点的状态由
                                   Leader 降级为 Follower, 进入新一轮选主。
                    优点:
                        选举速度快、算法复杂度低、易于实现
                        
                        比 bully 算法要稳定, 这是因为当有新节点加入或节点故障恢复后，会触发选主，但不一定会真正切主，
                        除非新节点或故障后恢复的节点获得投票数过半，才会导致切主
                        
                    缺点；
                        要求系统内每个节点都可以相互通信, 且需要获得过半的投票数才能选主成功, 通信量大
                                   
                    应用:
                         google 的 Kubernetes 的容器管理与调度,就是使用了开源的 etcd 组件, etcd 的集群管理器 etcds，
                         是一个高可用、强一致性的服务发现存储仓库, 就是采用了 Raft 算法来实现选主节点和一致性的
                         
            (3) ZAB 算法
                    概念: ZAB（ZooKeeper Atomic Broadcast）选举算法, 相较于 Raft 算法的投票机制, 增加了节点 ID 和数据 ID 作为
                    　　　参考进行选主, 节点 ID 和数据 ID 越大，表示数据越新，优先成为主
                    
                          每个节点都有一个唯一的三元组 (server_id, server_zxID, epoch)
                                 server_id:   表示本节点的唯一 ID
                                 server_zxID: 表示本节点存放的数据 ID, 数据 ID 越大表示数据越新，选举权重越大；
                                 epoch:       表示当前选取轮数，一般用逻辑时钟表示
                                 
                          ZAB 算法选主的原则是：server_zxID 最大者成为 Leader；若 server_zxID 相同，则 server_id 最大者成为 Leader
                    
                   角色分类:
                            Leader: 即主节点，同一时刻只有一个 Leader，负责协调和管理其他节点
                            Follower: Leader 的跟随者
                            Observer: 观察者，无投票权
                        
                   节点状态:
                            Looking 状态: 选举状态, 当节点处于该状态,认为当前集群中没有 Leader，因此自己进入选举状态
                            Leading 状态: 即领导者状态，表示已经选出主，且当前节点为 Leader
                            Following 状态: 即跟随者状态，集群中已经选出主后，其他非主节点状态更新为 Following, 表示对 Leader 的追随
                            Observing 状态: 即观察者状态，表示当前节点为 Observer，持观望态度，没有投票权和选举权
                            
                   过程:
                           第一步: 当系统刚启动时，3 个服务器当前投票均为第一轮投票，即 epoch=1, 且 zxID(数据 ID) 均为 0
                                  此时每个服务器都推选自己, 并将选票信息 <epoch, vote_id(服务器 id), vote_zxID(数据 ID)> 广播出去
                                  
                           第二步: 根据判断规则, 由于 3 个 Server 的 epoch、zxID(数据 ID) 都相同，因此比较 server_id,
                                  较大者即为推选对象, 因此 Server 1 和 Server 2 将 vote_id 改为 3，
                                  更新自己的投票箱并重新广播自己的投票
                                  
                           第三步: 此时系统内所有服务器都推选 Server 3, 因此 Server 3 当选 Leader，处于 Leading 状态，
                                  向其他服务器发送心跳包并维护连接；Server1 和 Server2 处于 Following 状态
                                  
                   优点:
                        选举稳定性比较好，当有新节点加入或节点故障恢复后，会触发选主，但不一定会真正切主，
                        除非新节点或故障后恢复的节点数据 ID 和节点 ID 最大，且获得投票数过半，才会导致切主
                        
                   缺点:
                        1. 采用广播方式发送信息, 集群中信息量为 n*(n-1) 个消息,容易出现广播风暴
                        2. 每个节点需要保存所有节点的 Id 和 数据 Id, 以便用于投票, 并且选举时间长                       
```
 
### 分布式协调与同步 --- 分布式共识
```shell
    1. 传统的分布式共识方法是基于多数投票策略来实现, 这个对于分布式在线记账不适用, 如果采用多数投票策略, 则记账权通常会完全掌握在
    　 主节点的手里, 这使得主节点非常容易造假，且存在性能瓶颈
    2. 概念
            (1) 分布式共识就是在多个节点均可独自操作或记录的情况下, 使得所有节点针对某个状态达成一致的过程
            (2) 分布式共识技术就是区块链技术共识机制的核心
            (3) 分布式共识包括两个关键点，第一获得记账权, 第二所有节点或服务器达成一致
    3. 分布式共识技术
            (1) PoW（Proof-of-Work，工作量证明）
                    概念: 是一种使用工作量证明机制的共识算法, 以每个节点的计算能力来竞争记账权的机制
                    原理: 利用区块的 index, 前一个区块的哈希值、交易的时间戳、区块数据和 nonce 值(计算不满足情况 nonce 递增)
                         通过 SHA256 哈希算法计算出一个哈希值，并判断前 k 个值是否都为 0(即拿到的哈希值的前 k 值是否都为 0),
                         如果不是，则递增 nonce 值，重新按照上述方法计算；如果是，则本次计算的哈希值为要解决的题目的正确答案。
                         谁最先计算出正确答案，谁就获得这个区块的记账权
                         
                    过程：
                         第一步: 客户端 A 产生新的交易，向全网进行广播，要求对交易进行记账(广播交易)
                         第二步: 每个记账节点接收到这个请求后，将收到的交易信息放入一个区块中, 通过 PoW 算法,
                                计算本节点的区块的哈希值，尝试找到一个具有足够工作量难度的工作量证明
                         第三步: 若节点 D 找到了一个工作量证明向全网广播(广播区块). 当且仅当包含在该区块中的交易都是有效且之前未
                                存在过的，其他节点才会认同该区块的有效性
                         第四步: 其他节点接收到广播信息后，若该区块有效，接受该区块，并跟随在该区块的末尾，制造新区块延长该链条
                         
                    优点:
                        PoW 的容错机制高, 允许全网 50% 的节点出错
                    
                    缺点:
                        每次达成共识需要全网共同参与运算, 共识达成的周期长、效率低，资源消耗大
                        
                    场景:
                        比特币
                        
            (2) PoS（Proof-of-Stake，权益证明）
                    概念；　由系统权益代替算力来决定区块记账权，拥有的权益越大获得记账权的概率就越大.
                           权益是指每个节点占有货币的数量和持币时间.
                           
                    原理；　PoS 是根据节点拥有的股权或权益进行计算的, 通过这个来获得区块记账权.
                           例如: 一种算法是 (持币数 * 持币时间 * 5% ）/365
                           
                    优点:
                           PoS 不需要消耗大量的电力就能够保证区块链网络的安全性, 缩短了达成共识所需要的时间.
                           
                    缺点:
                           PoS 算法中持币越多或持币越久，币龄就会越高，持币人就越容易挖到区块并得到激励，而持币少的人基本没有机会，
                           这样整个系统的安全性实际上会被持币数量较大的一部分人掌握，容易出现垄断现象
                           
                    场景:
                        以太坊
                           
            (3) DPoS（Delegated Proof of Stake，委托权益证明）
                    概念: 主要是为了解决垄断的问题, 通常会选出 k 个受托节点, 它们的权益都是完全相等的, 受托节点之间争取记账权
                         是根据算力进行竞争的, 只要受托节点提供的算力不稳定，计算机宕机,随时有后备的受托节点获取记账权.
                         
                    优点: 
                        1. 投票选举出的信誉度更高的受托人记账, 解决了所有节点均参与竞争导致消息量大、达成一致的周期长的问题
                           即 DPoS 能耗更低，具有更快的交易速度
                        2. 每隔一定周期会调整受托人，避免受托人造假和独权
                        
                    缺点:
                        1. 一旦出现故障节点，DPoS 无法及时做出应对，导致安全隐患
                        2. 持币人投票的积极性不高
                        
                    场景:
                        比特股
                        
    4. 总结
            (1) 一致性是 分布式系统中的多个节点之间，给定一系列的操作，在约定协议的保障下，对外界呈现的数据或状态是一致的, 强调的是结果
            (2) 共识 分布式系统中多个节点之间，彼此对某个状态达成一致结果的过程, 强调的是达成一致的过程
            (3) 共识算法是保障系统满足不同程度一致性的核心技术.  
```
 
### 分布式协调与同步 --- 分布式事务(All or nothing)
```shell
    1. 概念
           (1) 在分布式领域中, 电商平台一般有订单操作和减库存操作,可能这两个业务在不同的服务器上.
           　　只有保证这两个操作都执行完, 才能保证交易正常. 这个就涉及到分布式事务问题
           (2) 分布式事务: 就是在分布式系统中运行的事务，由多个本地事务组合而成
           (3) 特性
                    a. 原子性(Atomicity),  即事务最终的状态只有两种，全部执行成功和全部不执行
                    b. 一致性(Consistency), 是指事务操作前和操作后, 分布式系统中数据的完整性保持一致. 
                    　　　　　　　　　　　　　这里的一致性是强一致性, 也就是所有操作均执行成功，才提交最终结果，以保证数据一致性或完整性
                                        
                    c. 隔离性(Isolation), 是指当系统内有多个事务并发执行时，多个事务不会相互干扰，即一个事务内部的操作及使用的数据,
                                         对其他并发事务是隔离的
                    d. 持久性（Durability）, 一个事务完成后对数据的更新, 持久化到数据库中
                    
           (4) 刚性事务, 遵循 ACID 原则, 具有强一致性. 比如,数据库事务
               柔性事务, 根据不同的业务场景使用不同的方法实现最终一致性, 可以容忍一定时间内的数据不一致. 遵循 BASE 理论
               
           (5) BASE 理论
                    基本可用(Basically Available) : 分布式系统出现故障时, 允许损失一部分功能的可用性. 如某些电商 618 大促的时候
                                                   会对一些非核心链路的功能进行降级处理
                                                   
                    柔性状态（Soft State）: 在柔性事务中, 允许系统存在中间状态(但最后要最终一致性),且这个中间状态不会影响系统整体可用性
                                          如数据库读写分离，写库同步到读库（主库同步到从库）会有一个延时，其实就是一种柔性状态
                                          
                    最终一致性（Eventual Consistency）: 事务在操作过程中可能会由于同步延迟等问题导致短时间不一致, 但最终状态下,
                                                      数据都是一致的
                    
    2. 实现
            (1) 基于 XA 协议的二阶段提交方法
                    概念
                        a. 该方法是采用强一致性, 遵从 ACID 原则.
                        b. XA 协议是分布式事务协议, 规定了事务管理器和资源管理器接口. 事务管理器作为协调者, 负责各个本地资源的提交和回滚
                           资源管理器就是分布式事务的参与者，通常由数据库实现，比如 Oracle、DB2 等商业数据库都实现了 XA 接口
                        c. 二阶段提交协议用于保证分布式系统中事务提交时的数据一致性, 需要引入协调者(事务管理器)来管理所有的节点, 
                           并确保这些节点正确提交操作结果，若提交失败则放弃事务.
                           
                    过程
                        基本思路: 协调者下发请求事务操作，参与者将执行请求的操作结果通知协调者，协调者再根据所有参与者的反馈结果决定各
                        　　　　　参与者是要提交操作还是撤销操作
                        
                        第一步: 投票（voting）阶段(这一步参与者会执行事务, 但不提交)
                                    协调者(Coordinator, 即事务管理器)会向事务的参与者(Cohort, 即本地资源管理器)发起执行操作的
                               CanCommit 请求, 并等待参与者的响应. 参与者接收到请求后, 会执行请求中的事务操作, 并记录日志信息但不提交,
                               待参与者执行成功, 则向协调者发送“Yes”消息，表示同意操作；若不成功，则发送“No”消息，表示终止操作
                               
                        第二步: 提交（commit）阶段
                                    协调者会根据所有参与者返回的信息再向参与者发送 DoCommit 或 DoAbort 指令. 
                                    
                                    如果协调者收到的都是 “Yes”消息, 则向参与者发送“DoCommit”消息, 参与者会完成剩余的操作并释放资源,
                                    然后再向协调者返回 “HaveCommitted”消息. 如果协调者收到的消息中包含“No”消息, 则向所有参与者
                                    发送“DoAbort”消息, 之前发送“Yes”的参与者则会根据之前执行操作时的回滚日志对操作进行回滚, 
                                    然后所有参与者会向协调者发送 “HaveCommitted”消息.
                                    
                                    协调者接收到“HaveCommitted”消息，就意味着整个事务结束了
                                    
                    问题:
                        1. 同步阻塞问题: 所有参与节点都是事务阻塞型, 即当本地资源管理器占有临界资源时, 其他资源管理器如果要访问
                                       同一临界资源，会处于阻塞状态
                        2. 单点故障问题
                                一旦事务管理器(协调者)发生故障, 整个系统都处于停滞状态. 尤其是在提交阶段, 一旦事务管理器发生故障, 
                           资源管理器会由于等待管理器的消息, 而一直锁定事务资源, 导致整个系统被阻塞
                        3. 数据不一致问题
                                在提交阶段, 当协调者向参与者发送 DoCommit 请求之后, 如果发生了局部网络异常，
                          或者在发送提交请求的过程中协调者发生了故障，就会导致只有一部分参与者接收到了提交请求并执行提交操作，
                          但其他未接到提交请求的那部分参与者则无法执行事务提交,整个分布式系统便出现了数据不一致的问题
                          
                    特点:
                        强一致性, 同步执行, 算法简单易实现
                           
            (2) 三阶段提交方法(Three-phase commit protocol，3PC)
                    概念
                        a. 该方法是采用强一致性, 遵从 ACID 原则.
                        b. 为了解决两阶段提交的同步阻塞和单点故障问题, 采用了超时机制和准备阶段(预提交阶段)
                                超时机制: 在协调者和参与者中引入超时机制, 在规定的时间内没有收到对方的响应, 就会根据当前的状态选择
                                　　　　　提交或则终止整个事务.
                                预提交阶段: 在提交阶段之前, 加入预提交阶段, 在预提交阶段排除不一致的情况, 保证在最后提交之前各个参与
                                　　　　　　节点的状态是一致的.
                                
                    过程:
                        第一步: CanCommit 阶段(这一步部不执行事务)
                                    参与者会根据自身情况, 比如自身空闲资源是否足以支撑事务、是否会存在故障等，预估自己是否可以执
                                    行事务，但不会执行事务，参与者根据预估结果给协调者返回 Yes 或者 No 消息
                                    
                        第二步: PreCommit 阶段(执行事务,不提交)
                                    协调者根据参与者的回复情况, 来决定是否可以进行 PreCommit 操作
                                    (1) 如果所有参与者回复的都是“Yes”，那么协调者就会执行事务的预执行
                                            a. 发送预提交请求. 协调者向参与者发送 PreCommit 请求，进入预提交阶段
                                            b. 事务预提交.
                                                    参与者接收到 PreCommit 请求后执行事务操作, 并将 Undo 和 Redo 信息记录到
                                                    事务日志中
                                            c. 响应反馈
                                                    如果参与者成功执行了事务操作, 则返回 ACK 响应，同时开始等待最终指令
                                                    
                                    (2) 假如任何一个参与者向协调者发送了“No”消息, 或者等待超时之后，协调者都没有收到参与者的响应,
                                        就执行中断事务的操作
                                            a. 发送中断请求. 协调者向所有参与者发送“Abort”消息
                                            b. 终断事务. 参与者收到“Abort”消息之后，或超时后仍未收到协调者的消息, 执行事务的终断操作
                                            
                        第三步: DoCommit 阶段
                                    正常执行提交阶段:
                                        a. 发送提交请求. 协调者接收到所有参与者发送的 Ack 响应(PreCommit 阶段的 ACK 响应),
                                                       从预提交状态进入到提交状态, 并向所有参与者发送 DoCommit 消息
                                        b. 事务提交. 参与者接收到 DoCommit 消息之后, 正式提交事务. 完成事务提交之后, 
                                                    释放所有锁住的资源
                                        c. 响应反馈. 参与者提交完事务之后, 向协调者发送 Ack 响应
                                        d. 完成事务. 协调者接收到所有参与者的 Ack 响应之后, 完成事务
                                        
                                        
                                    事务中断阶段:
                                        a. 发送中断请求. 协调者向所有参与者发送 Abort 请求
                                        b. 事务回滚. 参与者接收到 Abort 消息之后，利用其在 PreCommit 阶段记录的 Undo 信息执行
                                                    事务的回滚操作，并释放所有锁住的资源.
                                        c. 反馈结果. 参与者完成事务回滚之后, 向协调者发送 Ack 消息
                                        d. 终断事务. 协调者接收到参与者反馈的 Ack 消息之后，执行事务的终断，并结束事务
                                        
                    问题:
                        1. 和二阶段提交方法一样, 三阶段提交方法需要锁定资源, 这样降低系统性能
                        2. 数据不一致
                        3. 性能较低, 系统吞吐量不高
                        
                    特点:
                        强一致性, 同步执行, 无同步阻塞问题, 无单点故障问题
             
            (3) 基于分布式消息的最终一致性方案
                    概念:
                        1. 使用案例, 在 eBay 的分布式系统架构中, 解决一致性问题的核心思想就是：将需要分布式处理的事务通过消息或者
                           日志的方式异步执行，消息或日志可以存到本地文件、数据库或消息队列中，再通过业务规则进行失败重试.
                           
                           最主要的是各个系统通过消息中间件来达到数据的最终一致性.
                           
                        2.  特点:
                                最终一致, 执行方式是异步执行的,无同步阻塞问题, 无单点故障问题, 故障高
                                
                        3. 缺点:
                                算法复杂度高.
                                
                                
    3. 二阶段提交协议与三阶段提交协议
            (1) 在 2PC 中, 如果所有参与者都返回结果后，会进入第二阶段，也就是提交阶段(执行阶段), 根据第一阶段的投票结果，进行提交或取消
                在 3PC 中, 进入真正的提交阶段前, 还会有一个预提交阶段，这个预提交阶段不会做真正的提交, 而是会将相关信息记录到事务日志中,
                当所有参与者都返回 Yes 消息后，才会真正进入提交阶段
                
            (2) 2PC 存在同步阻塞的问题, 如果协调者发生了故障, 那么参与者就一直阻塞等待协调者的事务命令操作. 3PC 增加了双方的超时时间,
                一定程度上减少了阻塞问题
            (3) 数据不一致问题,也是出现协调者上, 现在有 10 个参与者，协调者在发送事务操作信息的时候，在发送给了 5 个参与者之后发生了
                协调者故障。未收到信息的 5 个参与者会发生阻塞，收到信息的 5 个参与者会执行事务，以至于这 10 个参与者的数据信息不一致.
                
                3PC 引入与提交阶段, 相对 2PC 增加第一阶段预判断, 在一定程度减少故障导致的数据不一致问题
                
            (4) 在实际使用中，通常采用多数投票策略来代替第一阶段的全票策略，比如 Raft 算法等
                                       
```
 
### 分布式协调与同步 --- 分布式锁
```shell
    1. 概念
            (1) 分布式锁是被存在公共存储（比如 Redis、Memcache、数据库等三方存储中）, 确保不同机器上的服务都能互斥的访问到.
            (2) 分布式锁是实现分布式互斥的一种手段或方法.
            
    2. 实现
            (1) 基于数据库(关系型数据库)实现分布式锁
                    原理: 创建一张锁表, 要锁住某个资源时, 就在该表中增加一条记录, 想要释放锁就删除这条记录. 数据库对共享资源做了
                          唯一性约束, 如果有多个请求被同时提交到数据库的话，数据库会保证只有一个操作可以成功,
                          操作成功的那个线程就获得了访问共享资源的锁, 可以进行操作.
                          
                    使用场景: 因为频繁操作数据库, 使得磁盘 IO 开销大, 这种基于数据库实现分布式锁适用于并发量低，对性能要求低的场景
                    
                    问题: 
                         a. 单点故障问题. 数据库有问题, 其分布式锁出问题, 会导致系统不可用. 
                         b. 死锁问题. 数据库锁没有失效时间, 未获得锁的进程只能一直等待已获得锁的进程主动释放锁. 
                                     一旦已获得锁的进程挂掉或者解锁操作失败, 会导致锁记录一直存在数据库中，其他进程无法获得锁
                                     
            (2) 基于缓存实现分布式锁
                    原理: 可以通过 Redis 来实现分布式锁, 使用的是 setnx(key, value),  key 表示锁 id, 
                         value = currentTime + timeOut, 表示当前时间 + 超时时间。某个进程获得 key 这把锁后, 
                         如果在 value 的时间内未释放锁，系统就会主动释放锁. 根据 setnx 返回值进行相关的业务操作
                            返回 1, 说明该服务器成功获得锁, setnx 将 key 对应的 value 设置为当前时间 + 锁的有效时间. 某个服务在
                            　　　　拿到锁之后, 成功操作临界资源, 再删除这个锁的 key 进行释放锁.
                            返回 0, 说明其他服务器已经获得了锁，进程不能进入临界区. 该服务器可以不断尝试 setnx 操作，以获得锁。
                            
                         如果有个服务已经成功设置锁, 但是该服务出现异常, 没法正常释放锁, 那只能等到超时时间后, Redis 自动释放了
                         
                    优点:
                        (1) 性能更好. 数据存放在内存,避免频繁的磁盘 IO 操作
                        (2) 避免单点故障, 缓存可以跨集群部署
                        (3) 分布式服务有现成的分布式锁方法, 例如 Redis 的 setnx 方法
                        (4) 可以直接设置超时时间来控制锁的释放,  缓存服务器一般支持自动删除过期数据
                        
                    缺点:
                        (1) 锁失效时间的控制不稳定
                        (2) 可靠性不如 ZooKeeper
                        
                    基于队列来维持进程访问共享资源先后顺序, 即当一个进程释放锁之后，队列里第一个进程可以访问共享资源, 这样避免的不断重试,
                    以及隔多久重试一次的考虑,还有保证每个进程请求的公平性.
                        
            (3) 基于 ZooKeeper 实现分布式锁
                    组成:
                         ZooKeeper 的树形数据存储结构主要由 4 种节点构成:
                                1. 持久节点. 这是默认的节点类型，一直存在于 ZooKeeper 中。
                                2. 持久顺序节点. 在创建节点时, ZooKeeper 根据节点创建的时间顺序对节点进行编号。
                                3. 临时节点. 当客户端与 ZooKeeper 断开连接后，该进程创建的临时节点就会被删除。
                                4. 临时顺序节点, 就是按时间顺序编号的临时节点(分布式锁中有使用)
                                
                    实现流程:
                            第一步: 在与该方法对应的持久节点 shared_lock 的目录下, 为每个进程创建一个临时顺序节点. 例如吹风机就是
                                   一个拥有 shared_lock 的目录, 当有人买吹风机时，会为他创建一个临时顺序节点
                                   
                            第二步: 每个进程获取 shared_lock 目录下的所有临时节点列表, 注册子节点变更的 Watcher, 并监听节点
                            
                            第三步: 每个节点确定自己的编号是否是 shared_lock 下所有子节点中最小的，若最小，则获得锁.
                                   例如, 用户 A 的订单最先到服务器, 因此创建了编号为 1 的临时顺序节点 LockNode1.
                                   该节点的编号是持久节点目录下最小的, 因此获取到分布式锁,可以访问临界资源，从而可以购买吹风机
                                   
                            第四步: 若本进程对应的临时节点编号不是最小的，则分为两种情况：
                                        a. 本进程为读请求，如果比自己序号小的节点中有写请求，则本进程等待；
                                        b. 本进程为写请求，如果比自己序号小的节点中有读请求，则本进程等待
                                        
    3. 总结
            (1) ZooKeeper 分布式锁的可靠性最高, 有封装好的框架, 很容易实现分布式锁的功能, 并且几乎解决了数据库锁和缓存式锁的不足,
                因此是实现分布式锁的首选方法
                
                ZooKeeper 不是分布式锁, 而是一个分布式的、提供分布式应用协调服务的组件。基于 ZooKeeper 的分布式锁是基于 ZooKeeper 的
                数据结构中的临时顺序节点来实现的
                
            (2) 设计思路
                    a. 互斥性, 在分布式系统环境下, 分布式锁保证一个资源或一个方法在同一时间只能被一个机器的一个线程或进程操作
                    b. 锁失效机制(锁超时机制), 防止死锁, 即使有一个进程在持有锁的期间因为崩溃而没有主动解锁, 也要保证后续其他进程
                       可以获得锁
                    c. 有高可用的获取锁和释放锁的功能, 且性能要好
                    
            (3) 分布式锁的羊群效应问题
                    a. 羊群效应问题是整个分布式锁的竞争过程中, 大量的“Watcher 通知”和“子节点列表的获取”操作重复运行,
                       并且大多数节点的运行结果都是判断出自己当前并不是编号最小的节点, 继续等待下一次通知, 而不是执行业务逻辑
                       
                    b. 解决方案
                            第一步: 在与该方法对应的持久节点的目录下，为每个进程创建一个临时顺序节点
                            第二步: 每个进程获取所有临时节点列表，对比自己的编号是否最小，若最小，则获得锁
                            第三步:  若本进程对应的临时节点编号不是最小的，则继续判断：
                                        若本进程为读请求，则向比自己序号小的最后一个写请求节点注册 watch 监听,当监听到该节点释放锁后，则获取锁；
                                        若本进程为写请求，则向比自己序号小的最后一个读请求节点注册 watch 监听，当监听到该节点释放锁后，获取锁。
                                        
            (4) 应用
                    在一个集中式集群中, 以 Mesos 为例，Mesos 包括 master 节点和 slave 节点, slave 节点启动后是主动去和 master 节
                    点建立连接, 但建立连接的条件是需要知道 master 节点的 IP 地址和状态等.  master 节点启动后会将自己的 IP 地址和状态
                    等写入 ZooKeeper 中，这样每个 slave 节点启动后都可以去找 ZooKeeper 获取 master 的信息。而每个 slave 节点与
                    ZooKeeper 进行交互的时候，均需要一个对应的客户端. 这个例子，说明存在多个节点对应的客户端与 ZooKeeper 进行交互
                    由于每个节点之间并未进行通信协商，且它们都是独立自主的, 启动时间、与 ZooKeeper 交互的时间、事务完成时间都是独立的,
                    因此存在多个节点对应的客户端在同一时间完成事务的这种情况。
```
 
## 分布式资源管理与负载调度
 
### 分布式体系结构--集中式结构(Master/Slave 架构)
```shell
    1. 概念
            集中式结构: 系统内的所有节点信息都存储在中央服务器, 外部的所有的请求都先由中央服务器进行统一调度给其他处理服务器
                       只需要中央服务器与各个节点有通信就可以
                       
    2. 应用
            (1) Google Borg(单层调度框架)
                    a. 在 Borg 中, 一个集群称为一个 Cell, 每个 Cell 里面有一个 Leader, 称为 BorgMaster, 即为中央服务器；
                       其他服务器为节点服务器或从服务器, 被称为 Borglet
                       
                    b. BorgMaster 由两个进程组成, 一个是 Borgmaster 主进程, 处理客户端的 RPC 请求, 比如任务的执行状态更新或者查询等；
                       也需要管理系统中所有实体的状态(比如, 服务器、任务等), 并负责和 Borglet 通信. 另外一个进程是 scheduler 进程,
                       scheduler 进程负责任务调度, 根据请求任务对资源的需求和当前 Borglet 所在服务器的资源情况进行匹配,
                       为任务寻找一个合适的节点服务器执行
                       
                    c. Borglet, 运行在每个节点机器的一个 agent, 负责任务的拉起、停止、重启等，并管理和搜集本服务器资源,
                       将任务的状态、服务器状态等信息上报给 BorgMaster. 而 BorgMaster 会周期性地轮询每个 Borglet,
                       以获取节点服务器的状态和资源信息等。
                       
                       
            (2) Kubernetes(单层调度框架)
                    a. Kubernetes 是 Google 开源的容器集群管理系统, 是用于自动部署、扩展和管理容器化应用程序的开源系统, 
                       在集群的节点上运行容器化应用,可以进行自动化容器操作, 包括部署,调度和在节点间弹性伸缩等
                    b. 一个 Kubernetes 集群, 主要由 Master 节点和 Worker 节点组成，以及客户端命令行工具 kubectl 和其他附加项
                    c. Master 由 API Server, Scheduler, Cluster State Store 和 Control Manger Server 进程组成
                            API Server：是所有 REST 命令的入口, 负责处理 REST 的操作, 执行相关业务逻辑
                            Scheduler ： 根据容器需要的资源以及当前 Worker 节点所在节点服务器的资源信息，自动为容器选择合适的节点服务器
                            Cluster State Store：集群状态存储，默认采用 etcd，etcd 是一个分布式 key-value 存储，主要用来做共享配置和服务发现
                            Control Manager：用于执行大部分的集群层次的功能，比如执行生命周期功能（命名空间创建和生命周期、
                                             事件垃圾收集、已终止垃圾收集、级联删除垃圾收集等）和 API 业务逻辑
                                             
                    d. worker 节点, 作为工作节点, 运行在从节点服务器, 其中有 kubelet 和 kube-proxy 核心组件，负责运行业务应用的容器
                            kubelet： 通过与 API Server 进行通信, 接收 Master 节点请求或命令(根据调度策略), 在 Worker 节点上
                                      管控容器（Pod）, 并管控容器的运行状态（比如，重新启动出现故障的 Pod）等. Pod 是 Kubernetes
                                      的最小工作单元，每个 Pod 包含一个或多个容器
                                      
                            kube-proxy：负责为容器（Pod）创建网络代理 / 负载平衡服务, 从 API Server 获取所有 Server 信息,
                                       并根据 Server 信息创建代理服务, 这种代理服务为 Service. Kube-proxy 主要负责管理 Service
                                       的访问入口, 即实现集群内的 Pod 客户端访问 Service, 或者是集群外访问 Service, 具有相同服务
                                       的一组 Pod 可抽象为一个 Service. 每个 Service 都有一个虚拟 IP 地址（VIP）和
                                       端口号供客户端访问
                                       
            (3) Mesos(双层调度框架)
                    a. Mesos 与 Google Borg 的区别
                            Borg 的 Master 直接对接用户应用, 也就是说用户可以向 Borg 的 Master 直接请求任务. 但 Mesos 不可以,
                            Mesos 只负责底层资源的管理和分配，并不涉及存储、 任务调度等功能, 因此 Mesos Master 对接的是 
                            Spark、Hadoop、Marathon 等框架, 用户的任务需要提交到这些框架上. Mesos 的任务调度框架是双层结构
                            
                    b. 组成
                            一个集群包括 Mesos Master 和多个 Mesos Agent. Mesos Master 运行在中央服务器, Mesos Agent 运行
                            在节点服务器上. 
                            
                            Mesos Master 负责收集和管理所有 Agent 所在服务器的资源和状态, 并且对接 Spark、Hadoop 
                            等框架, 将集群中服务器的资源信息告知给这些框架, 以便这些框架进行任务资源匹配和调度.
                            Mesos Master 通常采用一主两备的方式, 以方便故障处理和恢复。而 Mesos Master 的选主策略是 ZAB 算法
                            
                            Mesos Agent 负责任务的拉起、停止、重启等, 并负责收集所在服务器的资源 (比如 CPU、内存等) 信息和状态,
                            上报给 Mesos Master
                            
                    c. 优势
                            1. 效率, Mesos 对物理资源进行了逻辑抽象, 在应用层而不是物理层分配资源, 通过容器而不是虚拟机（VM）分配任务
                                    因为应用程序的调度器知道如何最有效地利用资源, 在应用层分配资源能够为每个应用程序的特殊需求做考量 
                                    通过容器分配任务则能更好地进行“装箱”
                                                        
                            2. 可扩展性, Mesos 可扩展设计的关键是两级调度架构, 其中 Framework 进行任务调度, Mesos Master 
                                        进行资源分配. 由于 Master 不必知道每种类型的应用程序背后复杂的调度逻辑, 不必为每个任务做调度
                                        可以用非常轻量级的代码实现，更易于扩展集群规模
                                        
                            3. 模块化. 每接入一种新的框架, Master 无需增加新的代码, 并且 Agent 模块可以复用
                            
                    d. Mesos 支持容器部署
                            Mesos 本身只负责资源管理, 不负责任务调度. 但 Mesos 可以对接不同的框架, Mesos + Marathon 可以支持
                            容器调度和部署. Marathon 支持容器的调度(Docker 和 cgroups), 将容器部署请求发给 Mesos Master,
                            Mesos Master 再将请求转发给 Mesos Agent, Mesos Agent 的执行器会将容器拉起
                            
    3. Master 与 Slave 保活
            通过 TCP 长连接和心跳机制协同进行.
```
 
### 分布式体系结构--非集中式结构
```shell
    1. 不存在中央服务器和节点服务器, 解决单点瓶颈和单点故障问题, 还提升了系统的并发度, 比较适合大规模集群的管理
    2. 应用
            (1) Akka 集群
                    a. 背景
                            Akka 框架基于 Actor 模型, 提供一个用于构建可扩展的、弹性的、快速响应的应用程序的平台. Actor 是一个封装
                       状态和行为的对象, 它接收消息并基于该消息执行计算. Actor 之间互相隔离，不共享内存, 但 Actor 之间可通过
                       交换消息（mail）进行通信（每个 Actor 都有自己的 MailBox）, 即 Actor 需要发送到对方的 MailBox 中, 对方
                       依次从 MailBox 收取消息
                       
                    b. 构成
                            Akka 集群是一个完全去中心化的分布式集群管理系统, 一个集群由多个节点组成, 每个节点都可以进行数据处理和
                            任务执行, 节点之间均可进行通信. 节点分为 Leader 节点和非 Leader 节点. 与非 Leader 节点相比, 
                            Leader 节点只是增加负责节点的加入和移除集群的功能, 其他跟非 Leader 节点都一样.
                            
                    c. 功能
                            1. 数据传输
                                    每个节点都是并发处理, 所以可能对同一资源数据进行操作, 要保证数据的一致性. 首先数据以谁为标准, 
                               采用的是谁的时间戳最新(数据最新), 就以谁为准. 数据一致性方法是采用 Gossip 协议(最终一致性), 
                               每个节点周期性地从自己维护的集群节点列表中随机选择 k 个节点, 将自己存储的数据信息发给这 k 个节点,
                               而这些 k 节点采用时间戳最新以谁为准的策略, 对收到的数据和本地数据进行合并, 迭代几个周期后,
                               集群中所有节点上的数据信息就一致了
                               
                            2. 集群组建及管理
                                    Akka 集群的每个节点启动后, 读取配置文件获取种子节点列表，然后开始组建集群：
                                    
                               如果本节点为首种子节点，则把自己加入到集群列表中，即以自己为中心构建集群；
                               
                               如果本节点为种子节点，则向首种子节点请求加入集群，当首种子节点回复同意消息后，可以加入集群,
                               否则不可加入集群；
                               
                               如果本节点为普通节点，则可以向任一种子节点（包括首种子节点）请求加入集群，收到同意后,
                               则加入集群,否则不可加入集群.
                               
                               加入到首种子节点或种子节点的节点信息,会通过 Gossip 协议的传播方式传播给当前已加入的所有节点,
                               以完成集群组建.当集群组建完成后, 就不存在种子节点与普通节点之分了，每个节点均可执行 Actor 应用程序
                               
            (2) Redis 集群(开源的高性能分布式 key-value 数据库)
                    a. 特性
                            支持数据的持久化, 支持多种数据结构, 支持数据的备份(Master/Slave 模式的数据备份)
                           
                    b. 简介
                        Redis 集群是去中心化结构,所有节点均可负责存储数据、记录集群的状态, 客户端可以访问到任一节点上, 集群节点同样能
                        自动发现其他节点, 检测故障的节点,并在需要的时候在从节点中推选出主节点
                        
                        数据的可靠性: 每个节点都存在主备, 即每台服务器都运行两个 Redis 服务, 分别为主备, 主故障后, 备升主
                        数据的分片存储: Redis 集群使用了哈希槽, 集群内置 16384 个哈希槽, 每个节点负责一部分哈希槽。
                                     当客户端要存储一个数据或对象时, 对该对象的 key 通过 CRC16 校验后对 16384 取模,
                                     也就是 HASH_SLOT = CRC16(key) mod 16384 来确定哈希槽的序号, 从而确定存储在哪个节点上。
                                     
                                     当前集群有 3 个节点，那么:
                                         节点 A 包含 0 到 5500 号哈希槽；
                                         节点 B 包含 5501 到 11000 号哈希槽；
                                         节点 C 包含 11001 到 16383 号哈希槽
                                         
            (3) Cassandra 集群
                   a. Cassandra 集群的系统架构是所有的节点都是同样的角色, 避免了单点故障, 各个节点的同步是通过 Gossip 协议来进行.
                   b. Cassandra 集群与 Redis 集群的区别
                           Redis 集群节点是某一范围的哈希槽. 而 Cassandra 集群中的节点是值单个哈希值,每次客户端随机选择集群中的
                      一个节点来请求数据, 对应收到请求的节点将 对应的 key 在一致性哈希环上定位出是哪些节点应该存储这个数据,
                      然后将请求转发到对应的节点上，并将对应若干节点的查询反馈返回给客户端.
                      
                      
    3. 保活机制
            (1) 和集中式架构不同, 非集中式没有 Master 和 Slave 之说, 如果每个节点间建立 TCP 长连接浪费资源
            (2) 非集中式架构中每个节点需要被 k 个节点监控心跳, Akka 中 k 的选取是如果 k 不是由用户自己设置的, 当节点总数 n 小于 6,
                k = n - 1；若节点总数 n 大于等于 6, b 一直设置为 5
            (3) Akka 框架中各个节点间的故障检测
                    Akka 集群是一个去中心化的架构, 在 Akka 中集群组建完成后, 每个节点拥有整个集群中的节点列表, 
               集群中每个节点被 k 个节点通过心跳进行监控，每个节点根据集群节点列表，计算哈希值（比如根据节点 ID 计算一个哈希值），然后基
               于哈希值，将所有节点组成一个哈希环（比如，从小到大的顺序）, 由于每个节点上的计算方法一致，虽然每个节点独立计算，但每个节点
               上维护的哈希环是一致的, 最后根据哈希环, 针对每个节点逆时针或顺时针选择 k 个临近节点作为监控节点,
               比如 k = 3, 节点 1 被节点 2、节点 3 和节点 4 通过心跳监控，当节点 2 发现节点 1 心跳不可达
                时, 就会标记节点 1 为不可达（unreachable）, 并且将节点 1 为不可达的信息通过 Gossip 传递给集群中的其他节点,
                这样集群中所有节点均可知道节点 1 不可达. 
                      
    
```
 
### 分布式调度架构--单体调度
````shell
    1. 概念
            一个集群中只有一个节点运行调度进程, 这个节点对集群中的其他节点具有访问权限，可以搜集其他节点的资源信息、节点状态等进行统一管理,
       同时根据用户下发的任务时对资源的需求, 在调度器中进行任务与资源匹配, 然后根据匹配结果将任务指派给其他节点。
       
    2. 组成
            (1) Scheduler 由 Resource State(各个节点资源状态管理) 和 Task Scheduling(任务调度)
            
    3. 应用(google borg 单体调度)
            (1) 当用户提交一个作业给 BorgMaster 后, BorgMaster 会把该作业保存到 Paxos 仓库(persistent store)中,
                并将这个作业的所有任务加入等待队列中。调度器(Scheduler)异步地扫描等待队列,将任务分配到满足作业约束且有足够资源的
                计算节点上, 其中 调度是以任务为单位的, 而不是以作业为单位
                
            (2) 调度算法
                    a. 可行性检查, 即从节点中找到满足约束条件的一组机器, 即根据任务资源需求(CPU, 内存)
                    b. 评分, 从一组中再找到最佳一个.可以根据 考虑如何最小化被抢占的任务数、 尽量选择已经下载了相同 package 的机器、
                            目标任务是否跨域部署、在目标机器上是否进行高低优先级任务的混合部署等
                            
                       常用的评价算法:
                            (1) 最差匹配(worst fit), 符合条件下的平局分配, 将任务尽量分散到不同的机器上。该算法的问题在于，
                                                    会导致每个机器都有少量的无法使用的剩余资源
                            (2) 最佳匹配(best fit),  把单个机器上的任务塞得越满越好, 这样就可以“空”出一些没有用户作业的
                                                    机器(它们仍运行存储服务), 来直接放置大型任务. 
                                                    
                                                    问题是最佳匹配策略不利于有突发负载的应用, 因为如果用户错误估计资源需求,
                                                    紧凑的装箱操作会对性能造成巨大的影响. 同时存在单点故障
                                                    
                       策略: 对于资源比较紧缺，且业务流量比较规律，基本不会出现突发情况的场景，可以选择最佳匹配算法；
                            如果资源比较丰富，且业务流量会经常出现突发情况的场景，可以选择最差匹配算法
                            
    4. 多个集群/数据中心通过集群联邦来实现单体调度.将多个集群联合起来工作, 核心思想是增加一个控制中心, 由它提供统一对外接口,
       多个集群的 Master 向这个控制中心进行注册, 控制中心会管理所有注册集群的状态和资源信息, 控制中心接收到任务后会根据任务和集群信息
       进行调度匹配, 选择到合适的集群后, 将任务发送给相应的集群去执行
       
    5. 优点:
            (1) 单体调度器可以很容易实现对作业的约束并实施全局性的调度策略, 因此适合批处理任务和吞吐量较大、运行时间较长的任务
            (2) 单体调度系统的状态同步比较容易且稳定, 这是因为资源使用和任务执行的状态被统一管理, 降低了状态同步和并发控制的难度
            
    6. 缺点:
            (1) 灵活是和可扩展性不高, 因为调度算法只能全部内置在核心调度器当中
            (2) 单体调度存在单点故障的可能性
            (3) 中央服务器的性能会限制调度的效率, 同时也会限制支持的任务类型, 不同的服务具有不同的特征, 对调度框架和计算的要求都不一样
                单体调度框架会随着任务类型增加而变得复杂.
                
    7. 场景
            单体调度适用于小规模集群
````
 
### 分布式调度架构--两层调度
```shell
    1. 概念
            两层调度结构对应的就是两层调度器, 资源的使用状态由中央调度器和第二层调度器共同管理, 中央调度器从整体上进行资源的管理与分配,
            将资源分配到第二层调度器, 再由第二层调度器负责将资源与具体的任务配对, 第二层调度可以有多个调度器，以支持不同的任务类型.
            
            两层调度器中的第一层调度器仍是一个经简化的中央调度器, 通常放在分布式集群管理系统中，而第二层调度则是由各个应用程序框架完成.
            第一层调度器负责管理资源并向框架分配资源, 第二层调度器接收第一层调度器分配的资源，然后根据任务和接收到的资源进行匹配
            
            
    2. 应用(Mesos)
            (1) Mesos 本身实现的调度器为第一层调度，负责资源管理，然后将第二层任务调度交给了框架完成.
                    a. 资源管理集群是由一个 Master 节点和多个 Slave 节点组成的集中式系统.每个集群有且仅有一个 Master 节点, 
                       负责管理 Slave 节点, 并对接上层框架；Slave 节点向 Master 节点周期汇报资源状态信息,
                       并执行框架提交的任务(Slave 里面有执行器（Executor）)
                       
                    b. 框架(Framework) 运行在 Mesos 上, 是负责应用管理与调度的“组件”，比如 Hadoop、Spark、MPI 和 Marathon 等,
                       不同的框架用于完成不同的任务，比如批处理任务、实时分析任务等. 框架主要由调度器（Scheduler）和
                       执行器（Executor）组成，调度器可以从 Master 节点获取集群节点的信息 ，执行器在 Slave 节点上执行任务
                       
            (2) 过程
                    第一步: 框架向 Mesos Master 注册
                    第二步: Mesos Slave 节点定期或周期向 Mesos Master 上报本节点的空闲资源
                    第三步: Mesos Master 的 Scheduler 进程收集所有节点的空闲资源信息, 并以 Resource Offer 的方式将空闲资源发送给
                           注册的框架
                    第四步: 框架的 Scheduler 接收到 Mesos 发送的资源后, 进行任务调度与匹配, 匹配成功后, 
                           将匹配结果下发给 Mesos Master，并由 Mesos Master 转发给相应节点的执行器执行任务(执行器在 slave 中)
                           
            (3) 第一层调度算法(资源分配算法)
                    原理: 为框架分配资源, 以支持多用户多框架, 需要将当前可用资源分配给哪些框架以及分配多少
                    
                    a. 最大最小公平算法(Max-min Fairness,MMF)
                            关键是分配的资源都是平均分配的, 不存在用户得到的资源超过自己需求的, 剩下的资源还会平均分配到其他需要的用户上.
                        直到剩余分配完成或则用户需要的完成.
                        
                        例如 现在有总量为 100 的空闲资源, 有 4 个用户 A、B、C、D 对该资源的需求量分别为（35，10，25，45）
                        分配流程如下所示：
                           第一步: 按照用户对资源的需求量升序排列，则 4 个用户的需求量为（B:10，C:25，A:35，D:45）。
                           第二步: 平均分配空闲资源. 资源空闲总量 100, 除以用户数 4, 则平均空闲资源量为 25；按照第一步中需求量分配后,
                                  用户还需要资源为（0, 0, 10, 20）,且用户 B 由于资源需求量小于 25, 因此会剩余资源. 
                                  此时空闲资源量为 15, 资源需求人数为 2
                          第三步:  重复第二步, 平均分配资源, 15/2=7.5, 即分别为用户 A 和 D 分配 7.5 份资源,
                                   此时用户资源需求量为（0，0，2.5，12.5），空闲资源量为 0，资源需求人数为 2。
                           所有资源已分配完，算法终止
                           
                    b. 主导资源公平算法(Dominant Resource Fairness, DRF)
                            原理: 同样的资源量, 尽可能地最大化所有用户中某个最小的主导资源占用率. 主导资源占用率是指在框架中当个任务
                                  需要的资源, 例如 CPU, 内存等占各自总资源量百分比, 哪类资源百分比大, 哪类就是主导资源占用率.
                                  
                            例如:
                                假设系统中的资源共包括 18 个 CPU 和 36 GB 内存，有 Framework A 和 Framework B 分别运行了两种任务
                                Framework A 运行的是内存密集型的任务, 每个任务需要 2 CPU, 8 GB内存, 而 Framework B 运行的是 CPU
                                密集型的任务, 每个任务需要 6 CPU, 2 GB内存.
                                
                                第一步：计算资源分配量
                                            假设 Framework A 分配的任务数为 x, Framework B 分配的任务数为 y, 那么 Framework A
                                       消耗的资源为{2x CPU, 8x GB}, Framework B 消耗的资源数为{6y CPU, 2y GB}, 分配给两个 
                                       Framework 的总资源量为（2x + 6y）个 CPU 和（8x + 2y）GB 内存
                                       
                                第二步：确定主导资源
                                            对于 Framework A 来说, 每个任务要消耗总 CPU 资源的 2/18, 内存资源的 8/36, 
                                       所以 Framework A 的主导资源为内存；对于 Framework B 来说, 每个任务要消耗总 CPU 资源的 
                                       6/18 和内存资源的 2/36, 因而 Framework B 的主导资源为 CPU.
                                       
                                第三步：DRF 算法的核心是平衡所有用户的主导资源占用率, 尽可能试图最大化所有用户中最小的主导资源占用率.
                                       
                                       
                    总结:
                        最大最小公平算法适用于单一类型的资源分配场景，而主导资源公平算法适用于多种类型资源混合的场景.
                        最大最小公平算法是从公平的角度出发, 为每个用户分配不多于需求量的资源.
                        主导资源公平算法是从任务出发，目的在于尽量充分利用资源使得能够执行的任务越多越好
                        
            (4) 缺点
                    两层调度的缺点是第二层调度只知道集群中的部分资源, 无法进行全局最优调度.
                        
    3. 扩展
            (1) Mesos 用容器隔离不同的业务, 使得它们运行时不会互相干扰
            (2) 两层调度是悲观并发调度, 在执行任务之前避免冲突
                    
```
 
### 分布式调度架构--共享状态调度
```shell
    1. 共享状态调度器: 这种架构基本上沿袭了单体调度器的模式, 通过将单体调度器分解为多个调度器, 每个调度器都有全局的资源状态信息, 
                     从而实现最优的任务调度, 提供了更好的可扩展性, 这种调度架构的多个调度器需要共享集群状态，包括资源状态和任务状态等
                     
    2. 模块分类
            (1) State Storage 模块（资源维护模块）负责存储和维护资源及任务状态, 以便 Scheduler 查询资源状态和调度任务
            (2) Resource Pool 即为多个节点集群, 接收并执行 Scheduler 调度的任务
            (3)  Scheduler 只包含任务调度操作，而不是像单体调度器那样还需要管理集群资源等
            
    3. 简要
            (1) 乐观并发调度, 强调事后检测, 在事务提交时检查是否避免了冲突：若避免，则提交；否则回滚并自动重新执行。
                            是在执行任务匹配调度算法后, 待计算出结果后再进行冲突检测
                            
            (2) 悲观并发调度, 强调事前预防, 在事务执行时检查是否会存在冲突. 不存在，则继续执行, 否则等待或回滚。
                            执行任务匹配调度算法前，通过给不同的 Framework 发送不同的资源，以避免冲突
            (3) 共享状态调度则是乐观并发调度.
            
    4. 应用(Omega 系统)
            State Storage 模块负责存储和维护资源及任务状态，里面有一个 Cell State 文件(主本), 记录着全局共享的集群状态, 同时
            每个调度器都包含一个私有的 Cell State 副本, 也就是拥有了一个集群资源状态信息的副本, 进而达到了共享集群资源状态信息的目的.
            所有资源分配决策都在调度器（Scheduler）中进行。每个调度器都可以根据私有的 Cell State 副本，来制定调度决策,
            调度器可以查看 Cell 的整个状态, 并申请任何可用的集群资源. 一旦调度器做出资源调度决策, 它就会在原子提交中更新本地的 
            Cell State 的资源状态副本. 若同时有多个调度器申请同一份资源, State Storage 模块可以根据任务的优先级, 
                选择优先级最高的那个任务进行调度。
                
            a. Omega 共享调度工作原理
                    Omega 涉及 Job 并发调度, 采用了传统数据库中的乐观锁(MVCC, Multi-Version Concurrency Control,
                    基于多版本的并发访问控制）, 即对每一个应用都发放所有的可用资源, 在更新集群状态时使用乐观并发控制来解决资源冲突问题,
                    来提高 Omega 的并发度.无论事务是否执行成功, 调度器都会在事务执行之后,重新从主本那里同步更新本地 Cell State 的
                    资源状态副本, 以保证本地集群信息状态的有效性. 若事务未成功执行, 则调度器会在必要时重新运行其调度算法并再次
                    尝试申请资源
                    
    5. 优点
            每个调度器可以在全局范围内进行任务分配, 提高调度的并发量
            
    6. 缺点
            很可能会产生资源冲突, 从而导致任务调度失败, 需要对调度失败的任务进行处理, 比如重新调度, 任务调度状态维护, 增加调度的复杂性.
            
```
## 分布式计算
### 相关概念
```shell
    1. 离线计算, 主要的应用场景是对时延要求不敏感、计算量大、需要计算很长时间（比如需要数天、数周甚至数月）的场景，比如大数据
                分析、复杂的 AI 模型训练（比如神经网络）等, 先采集数据, 并将这些数据存储起来, 待数据达到一定量或规
                模时再进行计算，然后将计算结果（比如离线训练的模型）应用到实际业务场景中
    2. 实时计算, 和离线计算相对应的,实时计算对时延的要求比较敏感, 需要短时间执行完成并输出结果, 比如秒级、分钟级,
                用于秒杀、抢购等场景, 其计算量通常不大、数据量也不会太多, 计算的数据往往是 K、M 级别
                
    3. 批量计算, 将原始数据集划分为多个数据子集, 然后每个任务负责处理一个数据子集，多个任务并发执行，以加快整个数据的处理, 
                例如 MapReduce
                
    4. 流式计算, 针对的小规模的流式数据, 通常用于商业场景中每天的报表统计、持续多天的促销活动效果分析
    5. 离线计算和实时计算是计算时延的维度进行分类, 批量计算和流式计算是从计算方式的维度进行分类
    
```
 
### 分布式计算--MR 模式(Map Reduce 模式)
```shell
    1. 概念
            (1) 分治法, 一个复杂的、难以直接解决的大问题, 分割成一些规模较小的、可以比较简单的或直接求解的子问题, 这些子问题之间相互独立
                       且与原问题形式相同, 递归地求解这些子问题, 然后将子问题的解合并得到原问题的解
                       
            (2) Hadoop MapReduce 是 Google 的开源实现, 是典型的分治法. 
            (3) 这种模式下任务运行完成之后, 整个任务进程就结束了, 属于短任务模式, 任务的启动和停止都是挺耗时的, 处理实时性不合适, 例如
                不太适合处理流数据任务.
            (4) MapReduce 模式处理方式是先将数据收集缓存到一定量才统一进行处理, 实时性不是很高.
            (5) 以特定数据类型(静态数据)为维度
            
    2. 抽象模型
            (1) 分为 Map 和 Reduce 两个阶段
            (2)  Map 阶段, 将大数据计算任务拆分为多个子任务, 拆分后的子任务通常具有如下特征：相对于原始任务来说, 划分后的子任务与原任务
                          是同质的，比如原任务是统计全国人口数, 拆分为统计省的人口数子任务时, 都是统计人口数, 子任务的数据规模和计
                          算规模会小很多, 多个子任务之间没有依赖，可以独立运行、并行计算
                          每个 Map 作业处理一个子任务需要调用多次 map() 函数来处理
                          
            (3) Reduce 阶段, 第一阶段拆分的子任务计算完成后, 汇总所有子任务的计算结果, 以得到最终结果. 例如, 汇总各个省统计的人口数,
                            得到全国的总人口数
    3. 工作原理
            (1) 步骤
                    第一步, User Program 将任务下发到 MRAppMaster 中, MRAppMaster 执行任务拆分步骤, 把 User Program 下发的任务
                           划分成 M 个子任务（M 是用户自定义的数值）. 假设 MapReduce 函数将任务划分成了 5 个,  Map 作业有 3 个,
                           Reduce 作业有 2 个；集群内的 MRAppMaster 以及 Worker 节点都有任务的副本
                    第二步, MRAppMaster 分别为 Mapper 和 Reducer 分配相应的 Map 和 Reduce 作业, 即 Map 作业 3 个, Reduce 作业
                           2 个
                    第三步, 被分配 Map 作业的 Worker, 开始读取子任务的输入数据, 并从输入数据中抽取出 <key, value> 键值对,
                           每一个键值对都作为参数传递给 map() 函数
                    第四步, map() 函数的输出结果存储在环形缓冲区 kvBuffer 中, 这些 Map 结果会被定期存到本地磁盘中, 这些被存储在 R 
                          个不同的磁盘区。R 表示 Reduce 作业的数量(由用户定义), 如, R=2. 每个 Map 结果的存储位置都会上报给
                          MRAppMaster
                    第五步, MRAppMaster 通知 Reducer 负责的作业在哪一个分区, Reducer 远程读取相应的 Map 结果, 即键值对.
                           当 Reducer 把它负责的所有键值对都读过来后, 首先根据键值对的 key 值进行排序, 将相同 key 值的键值对聚集
                           在一起, 从而有利于 Reducer 对 Map 结果进行统计
                           
                    第六步, Reducer 将具有相同 key 值的键值对合并，并将统计结果作为输出文件存入负责的分区中
                    
                   整个流程是 Input（输入）, Splitting（拆分）, Mapping（映射）, Reducing（化简）以及 Final Result（输出）.
                   通常 Mapping 与 Reducing 之间有个 Shuffling 操作(改组),  主要是对 mapping 之后的结果进行排序整合, 之后才
                   进行 Reducing 
                   
    4. 扩展
        (1) Fork-Join 是 Java 等语言或库提供的原生多线程并行处理框架，采用线程级的分而治之计算模式, 利用多核 CPU 的优势, 以递归的方式
            把一个任务拆分成多个“小任务”， 把多个“小任务”放到多个处理器上并行执行，即 Fork 操作。当多个“小任务”执行完成
            之后, 再将这些执行结果合并起来即可得到原始任务的结果，即 Join 操作, 不能大规模扩展, 值适用单个 Java 虚拟机运行, 各个小任务间
            可以相互通信
            
        (2) 而 MapReduce 可以大规模扩展, 通过 MapReduce 拆分后的任务, 可以跨多个计算机去执行，且各个小任务之间不会相互通信
```
 
### 分布式计算-- Stream
```shell
    1. 概念
            (1) 流数据, 是数据如流水般持续、快速地到达, 其具备海量数据规模, 可达 TB 级, 对实时性要求高, 随着时间流逝, 
                       数据的价值会大幅降低, 系统无法控制将要处理的数据元素的顺序
            (2) 流计算是实时获取来自不同数据源的海量数据,进行实时分析处理, 获得有价值的信息, 实时要求高, 流计算一般用于处理数据
                密集型应用
            (3) 以特定数据类型(动态数据)为维度
            
    2. stream 工作原理
            第一步: 提交流式计算作业, 这是一种常驻计算服务, 本身没有存储数据的功能, 处理逻辑不可更改, 作业中需要管理好处理节点的个数
                    以及数据转发的规则.
            第二步: 加载流式数据进行流计算, 流处理节点会对数据进行预定义的处理操作, 并在处理完后按照某种规则转发给后续节点继续处理.
            第三步: 持续输出计算结果
            
    3. 应用 Apache Storm
            (1) Storm 集群上有两种节点, 即主节点（Master Node）和工作节点（Worker Nodes）
                    主节点: 运行 "Nimbus" 的守护进程, 负责为工作节点分配任务以及进行故障监控, 一个 Storm 集群只有一个 Nimbus 进程
                    工作节点: 运行着 "Supervisor" 的守护进程, 负责接收 Nimbus 分配的任务, 并根据需要来启动和停止工作进程, 其中每个
                            工作进程都执行一个子任务
                            
            (2) Nimbus 与 "Supervisor" 协同
                    为了防止 Nimbus 单点故障, 引入 ZooKeeper 集群来增加可靠性,  Master Node 与 Worker Node 之间的交互通过
                ZooKeeper 完成, 例如 Nimbus 会将任务的分配情况或信息发送给ZooKeeper 集群, 然后 Supervisors 向 ZooKeeper 
                集群获取任务, 并启动工作进程以执行任务.差不多任务中包含 2 个组件, 一个是 spout 用于接收源数据, 将它们发送到拓扑中.
                另外一个是 Bolt 负责处理数据流, 只具备单一的计算逻辑, 如果数据流比较复杂, 可能有多个 Bolt 
```
 
### 分布式计算-- Actor
```shell
    1. 概念
            (1) Actor 计算模式是以计算过程或则处理过程的维度
            (2) Actor 模型, 代表一种分布式并行计算模型, 它规定了 Actor 的内部计算逻辑, 以及多个 Actor 之间的通信规则, 消息通信是采用
                异步的方式, 提高并发. 例如, 对象 A、B 和 C 对应着 Actor A、Actor B 和 Actor C，当 Actor A
                和 Actor B 需要执行 Actor C 中的 Function 逻辑时，Actor A 和 Actor B 会将消息发送给 Actor C,  Actor C 的
                消息队列存储着 Actor A 和 Actor B 的消息, 然后根据消息的先后顺序, 执行 Function 即可
    2. 三要素
            (1) 状态、行为和消息
                    状态(State) : Actor 本身的信息, 相当于 OOP 对象中的属性, 状态的改变受 Actor 自身行为的影响, 且只能被自己修改
                    行为(Behavior) : 指 Actor 的计算处理操作, 相当于 OOP 对象中的成员函数, 但是 Actor 的计算路基不能被其他 Actor
                                    调用, 只能是收到消息才会触发自身的计算行为.
                    消息(Mail): Actor 的消息以邮件形式在多个 Actor 之间通信传递，每个 Actor 会有一个自己的邮箱（MailBox）,
                               用于接收来自其他 Actor 的消息, Actor 模型中的消息也称为邮件 
                               
    3. Actor 特征
            (1) 更高级的抽象, 封装了状态和行为.
            (2) 非阻塞性, Actor 模型通过消息传递机制, 避免了阻塞.
            (3) 并发高, 每个 Actor 只需处理本地 MailBox 的消息, 因此多个 Actor 可以并行地工作, 从而提高整个分布式系统的并行处理能力
            
    4. 缺点
            (1) Actor 缺少继承和分层, 这使得即使多个 Actor 之间有公共逻辑或代码部分, 都必须在每个 Actor 中重写这部分代码, 重用性小.
            (2) Actor 模型不适用于对消息处理顺序有严格要求的系统
            
    5. 应用
            (1) Erlang/OTP, Erlang 是一种通用的、面向并发的编程语言, 使用 Erlang 编写分布式应用比较简单
            (2) Akka 框架基于 Actor 模型, 提供了一个用于构建可扩展的、弹性的、快速响应的应用程序的平台
            
    6. 扩展
            
```
 
### 分布式计算-- 流水式
```shell
    1. 概念
            (1) 流水线计算模式: 一个任务拆分为多个任务, 同时前一个任务的结果是另外一个任务的输入, 不同的步骤可以采用不同的进程执行
                              这使得不同任务可以并行执行，从而提高了系统效率
            (2) MapReduce 计算模式适合任务并行的场景, 而流水线计算模式适合同类型任务数据并行处理的场景
                              
    2. 应用(TensorFlow)
            (1) 输入流水线
                    a. 步骤
                            提取
                                通过多种途径读取数据, 比如内存, 本地的 SSD, 远程的 HDFS
                                
                            转换
                                使用 CPU 处理器对输入的数据进行解析以及预处理操作, 包括混合重排(shuffling), 批处理(batching),
                                以及一些特定的转换
                                
                            加载
                                将转换后的数据加载到执行机器学习模型的加速器设备上,例如 GPU 或 TPU
                                
                    b. 原理
                            当 CPU 对第 N 个样本的数据完成预处理之后, 会将预处理后的数据发送给 GPU, 然后 CPU 继续对第 N + 1 个样本
                       的数据进行预处理, 同时 GPU 对第 N 个样本数据进行模型训练.
                       
                    c. 优势
                            1. 提高 CPU, GPU 的利用率
                            2. 加速训练过程
                            
            
```
 
## 分布式通信
### 分布式通信--远程调用(rpc), 同步通信为主
```shell
    1. 概念
            (1) 本地调用: 进程内函数之间的相互调用
                远程调用: 是进程间函数的相互调用(IPC, Inter-Process Communication) 的一种方式
                
            (2) 进程是否部署在同一台机器上
                    a. 本地过程调用(Local Procedure Call, LPC), 运行在同一台机器上的进程之间的互相通信
                    b. 远程过程调用(Remote Procedure Call, RPC), 不同机器中运行的进程之间的相互通信, 如调用远程机器上的服务.
                    
            (3) 远程调用的核心是在网络服务层封装了通信协议、序列化、传输等操作，让用户调用远程服务如同进行本地调用一样
                一般用在同步远程调用比较多.
                    
    2. RPC
            (1)  RPC 的核心是 在用户眼里, 远程过程调用和调用一次本地服务没什么不同, 例如订单系统进程并不需要知道底层是如何传输的. 
                 其实 rpc 调用实际上是先调用机器 A 本地的函数, 再将这个函数名和参数进行序列化后,通过本地网络系统调用发送给远程服务的机器B,
                 其再进行反序列化, 再通过取函数名和参数,调用这个远程的函数, 然后其返回值再通过序列化后传回给机器 A, 机器 A 收到后,进行
                 反序列化拿到远程调用的结果.
                 
                 RPC 是指调用方通过参数传递的方式调用远程服务, 并得到返回的结果
                 
            (2) rpc 与本地过程调用函数区别
                    a. 调用 ID 和函数的映射关系
                            如果是本地过程调用函数是通过函数名, 这个函数名在一个进程中就是一个指针地址, 而 rpc
                       则是不同进程间的调用, 无法用函数名进行传递, 需要调用 ID, 而这个调用 ID 需要在所有机器上都是唯一的, 本地调用
                       的机器和远程服务调用的机器, 函数与调用 ID 映射要保存一致
                       
                       当一台机器 A 上运行的进程 P 需要远程调用时，它就先查一下机器 A 维护的映射表, 找出对应的调用 ID, 然后把它传到
                       另一台机器 B 上, 机器 B 通过查看它维护的映射表，从而确定进程 P 需要调用的函数，然后执行对应的代码，
                       最后将执行结果返回到进程 P
                       
                    b. 序列化和反序列化
                            本地过程调用函数只需要通过共享内存, 例如把参数压入栈中, 进程间就可以拿到数据. 而 rpc 远程过程调用则因为
                        各个机器的大小端不同, 需要进行序列化和反序列化. 
                        
                    c. 网络传输协议(TCP 等)
                    
            (3) 应用
            
                (Dubbo)
                    a. 架构
                            服务提供方: 服务提供方会向服务注册中心注册提供的服务
                            服务注册中心: 服务注册与发现中心, 负责存储和管理"服务提供方"注册的服务信息和"服务调用方"订阅的服务类型等
                            服务调用方: 根据服务注册中心返回的服务所在的地址列表, 通过远程调用访问远程服务
                            监控中心: 统计服务的调用次数和调用时间等信息的监控中心, 以方便进行服务管理或服务失败分析等
                            
                    b. 流程
                            第一步: "服务提供方"需要向"服务注册中心"注册提供的服务
                            第二步: "服务调用方"向"注册中心"预订调用服务的提供方地址列表
                            第三步: "服务注册中心"将服务对应的提供方地址列表返回给"调用方"
                            第四步: "服务调用方"根据服务地址信息进行远程服务调用
                            5. 服务调用方和服务提供方定时向监控中心发送服务调用次数及调用时间等信息
                            
                          
                  
                (RMI)
                    a. 原理
                            RMI 是基于对象的, 充分利用了面向对象的思想去实现整个过程, 其本质就是一种基于对象的 RPC 实现
                       RMI 通过对象作为远程接口来进行远程方法的调用, 返回的结果也是对象形式, 可以是 Java 对象类型, 
                       也可以是基本数据类型
                       
                    b. 适用范围
                            需要 Java 环境
                                         
```
 
### 分布式通信--发布订阅
```shell
    1. 概念
            (1) 一般为异步通信
            (2) 消息系统中有 2 种典型的模式
                    a. 点对点模式(P2P, Point to Point), 生产者发送消息到消息中心, 当一个消费者消费到消息后, 消息不再存储, 其他消费者
                    　　　　　　　　　　　　　　　　　　　　拿不到消息. 即点对点模式虽然支持多个消费者, 但一个消息只能被一个消费者消费,
                                                      不允许重复消费
                    b. 发布订阅模式, 生产者可以发送消息到消息中心, 而消息中心通常以主题(Topic)进行划分，每条消息都
                                   会有相应的主题, 消息会被存储到自己所属的主题中, 订阅该主题的所有消费者均可获得该
                                   消息进行消费
                                   
    2. Kafka 消息系统
            (1) Kafka的系统架构 除了生产者(Producer), 消息中心(Broker, kafka), 消费者(Consumer) 外, 还需要 ZooKeeper
                ZooKeeper 集群可以看作是一个提供了分布式服务协同能力的第三方组件, Consumer 和 Broker 启动时均会向 ZooKeeper 进行
                注册, 由 ZooKeeper 进行统一管理和协调.
                
            (2) Kafka 中的主题是一个逻辑概念, 指的是消息的类型或则数据类型. 分区是针对主题而言的, 指的是一个主题的内容可以被划分成
                多个分区, 分布在不同的 Broker 上, 而不同的 Broker 分布在不同的节点上, 一个总的 topic 可以分布在不同的 Broker 上
                只不过是 topic 内不同分区分布在不同 Broker 上, 某一 topic 的同一个分区只属一个 Broker.
                
                分区的好处, 既可以进行负载均衡 topic 分为不同分区分布在不同 Broker, 也可以进行消息备份, 即 2 个分区内容是一致.
                
            (3) 消费组, 指的是多个消费者的一个集合。一个消费组中的消费者一起消费同一主题消息, 并且主题中每个消息只可以由消费组中的某一个
                       消费者进行消费
                       
                引入消费组的作用是, 防止当个消费者处理能力有限, 可能会导致 Broker 溢出, 导致消息丢弃.
                
            (4) Kafka 中如果在消息中心堆积太多消息超过一定数量, 就是将消息抛掉.
                
    3. 优点
            (1) 实现系统解耦，易于维护. 生产者只管发布消息, 消费者只管消费消息.
            
            
    4. 扩展
            (1) 观察者模式是定义了被观察者与观察者的直接交互或通信关系, 是直接通信方式.观察者和被观察者通信时延会低一些，但它们的依
                赖关系比较强，不管是被观察者还是观察者逻辑或接口有更改，另外一个均会受影响
                
                而发布订阅模式采用消息中心, 实现间接通信
                
            (2) 发布订阅模式, 需要消费者提前向消息中心订阅消息，也就是说消息中心需要提前获取消费者信息, 比较适合消费者为长驻进程或
                             服务的场景
```
 
### 分布式通信--消息队列
```shell
    1. 概念
            (1) 消息队列是基于队列实现的, 存储具有特定格式的消息数据, 比如定义一个包含消息类型、标志消息唯一性的 ID、消息内容
            (2) 架构组成有生产者, 消息队列(一个先进先出的数据结构), 消费者.
            
    2. RocketMQ
            (1) RokcetMQ 包括 NameServer Cluster, Producer Cluster, Broker Cluster 和 Consumer Cluster 共 4 部分
                    a. NameServer Cluster: 名字服务器集群, 与 Kafka 中引入的 ZooKeeper 类似, 提供分布式服务的协同和管理功能,
                                           管理 Broker 的信息, 包括有哪些 Broker, Broker 的地址和状态等.
                    b. Producer Cluster: 指的是生产者集群, 负责接收用户数据, 然后将数据发布到消息队列中心 Broker Cluster,
                                         使用生产者集群目的是, 第一多个 Producer 可以并发接收用户的输入数据，提升业务处理效率
                                         第二考虑到单点故障和可靠性问题, 确保一个 Producer 故障后, 还能正常．
                    c. Consumer Cluster: 指的是消费者集群, 负责从 Broker(消息队列) 中获取消息进行消费. Consumer 集群可以提升
                    　　　　　　　　　　　　消费者的消费能力, 以避免消息队列中心存储溢出, 消息被丢弃
                    d. Broker Cluster : Broker 集群, 负责存储 Producer Cluster 发布的数据. Broker Cluster 中的每个 Broker 
                                       都进行了主从设计, 即每个 Broker 分为 Broker Master 和 Broker Slave. Master 可读可写, 
                                       Slave 只读. 每次 Broker Master 会把接收到的消息同步给 Broker Slave，以实现数据备份.
                                       
            (2) 过程
                    第一步: 首先启动 NameServer, 再启动 Broker. Broker 启动后, 会主动找 NameServer 建立连接, 并将自己的信息注册到
                           NameServer 上. 注册完毕后, Broker 会周期性地给 NameServer 发送心跳包, 比如每隔 1s 发送一次, 心跳包里
                           还可以包括 Broker 当前存储的数据信息
                           
                    第二步: 创建主题, 并确定这个主题的数据放入哪些 Broker 中
                    第三步: 当 Producer 生产消息要发送到主题时, 需要先到 NameServer 查询该主题存放在哪些 Broker 中,
                           获取到相关 Broker 信息后, 将消息发送给这些 Broker 进行存储
                    第四步: Consumer 要消费消息, 也需要先到 NameServer 查询一下该主题的消息存储在哪些 Broker 上, 然后去相应的 
                           Broker 获取消息进行消费
                           
    3. 应用场景
            (1) 消息队列对消费者没有特别需求, 比较适合消费者为临时用户的场景, 只要创建消费者时刻, 向消息队列取消息就行. 而不像
            　　发布订阅那样消费者需要提前向消息中心订阅主题.
            
    4. 发布订阅模式和消息队列模式系统解耦不同
            (1) 实现解耦的数据结构不同, 发布订阅模式的消息中心采用的是 map 方式存储. 消息队列则采用的是先进先出特征的队列结构.
            (2) 解耦的方式. 发布订阅模式需要消费者提前向消息中心订阅自己的主题, 待生产者发布消息到 Broker 后, 有　Broker 推给
            　　　　　　　　　对应的消费者. 而消息队列模式, 则临时创建消费者, 消费者再从消息中心拉取对应的数据.
```
 
## 分布式存储
### 概念
```shell
    1. CAP 理论
            (1) 概念
                    C 代表 Consistency, 一致性, 指所有节点在同一时刻的数据是相同的, 即更新操作, 执行结束并响应用户完成后，
                                       所有节点存储的数据会保持相同
                    A 代表 Availability, 可用性, 是指系统提供的服务一直处于可用状态, 对于用户的请求可即时响应
                    P 代表 Partition Tolerance, 分区容错性, 指在分布式系统遇到 网络分区的情况下, 仍然可以响应用户的请求. 网络分区是
                                                指因为网络故障导致网络不连通, 不同节点分布在不同的子网络中, 各个子网络内网络正常.
                                                例如,在电商系统中, 假设 C 与 A 和 B 的网络都不通了, A 和 B 是相通的, 即形成了两
                                                个分区{A, B}和{C}, 在这种情况下, 2 个分区分别可以处理用户的请求
                                                
            (2) 分布式系统无法同时满足 CAP 这三个特性, 通常在保证P(分区容错性)情况下, C(数据一致性), A(可用性), 只能 2 选 1.
                例如 2 个服务器分别有 DB1, DB2, 正常情况下 2 台服务器数据相互同步, 如果这 2 台服务器发生故障, 则如果保证数据一致性, 
                则但 DB1 已经有数据被更新, DB2 则一直阻塞等待数据被同步, 才能使服务器 2 正常工作.
                
            (3) CAP 选择策略
                    a. 涉及钱的交易时, 数据的一致性至关重要, 选择保 CP 弃 A
                    b. 剩下大多数情况, 保 AP 弃 C, 很多情况下不需要强一致性, 只要满足最终一致性就可以了
                    
            (4) 各个情况
                    a. 保 CA 弃 P, 网络分区(网络不连通)难以避免, 牺牲分区容错性 P，就相当于放弃使用分布式系统, 例如 关系型数据库 
                                   DBMS(比如 MySQL, Oracle)部署在单台机器上
                                   
                    b. 保 CP 弃 A
                            涉及到数据的强一致性, 有 Redis, HBase, ZooKeeper. 
                            ZooKeeper
                            
                                数据强一致性
                                    当用户向节点发送写请求时, 如果请求的节点刚好是 Leader, 那就直接处理该请求； 如果请求的是 
                                Follower 节点, 那该节点会将请求转给 Leader, 然后 Leader 会先向所有的 Follower 发出一个 
                                Proposal, 等超过一半的节点同意后, Leader 才会提交这次写操作, 从而保证了数据的强一致性.
                                
                                弱可用性
                                    当出现网络分区时, 如果其中一个分区的节点数大于集群总节点数的一半, 那么这个分区可以再选出一个 Leader,
                                仍然对用户提供服务, 但在选出 Leader 之前, 不能正常为用户提供服务；如果形成的分区中, 没有一个分区的节点数
                                大于集群总节点数的一半，那么系统不能正常为用户提供服务, 必须待网络恢复后, 才能正常提供服务.
                            
                    c. 保 AP 弃 C
                            该分布式场景需要很高的可用性, 在网络分区出现情况下, 可用允许最终一致性. 例如很多查询网站、电商系统中的
                            商品查询等, 用户体验非常重要
                            
                            
    2. 网络分区
            (0)
                    a. 网络分区是针对同一个集群, 当同一个集群跨多个网络时,容易触发网络分区, 比如一个业务集群部署在多个数据中心.
            (1) 网络分区指的是在分布式集群中, 节点之间由于网络不通, 导致集群中节点形成不同的子集, 子集中节点间的网络相通,
                而子集和子集间网络不通
            (2) 集中式架构的网络分区
                    a. 通常有 Master 节点和 Slave 节点, Master 节点有主节点, 备节点. Slave 节点与 Master 节点通信, 而 
                       Master 节点内主服务器与备服务器通信. 而集中式架构的网络分区是 Master 内的主节点与备节点网络不通, 
                       一部分 Slave 节点只能与主 Master 节点连通, 另一部分只能与备 Master 节点连通
                       
            (3) 非集中式架构的网络分区
                    a. 非集中式架构中, 节点是对称的, 非集中式架构的网络分区是形成不同子集, 子集内节点间可互相通信, 
                       而子集之间的节点不可通信
                       
            (4) 处理网络分区方法
                    a. 通过 Static Quorum 处理网络分区
                            Static Quorum 是固定票数策略, 在系统启动之前, 先设置一个固定票数. 当发生网络分区后, 如果一个分区中的
                       节点数大于等于这个固定票数, 则该分区为活动分区. 需要注意的是为了防止多个活动分区, 导致多主的问题.
                       固定票数的取值有约束, 固定票数 ≤ 总节点数 ≤ 2* 固定票数 - 1
                       
                       优点: 实现简单,容易
                       缺点:  
                            1. 当隔离的分区数少时, 容易选出一个唯一的活动分区(其活动分区内的数量满足条件), 但分区数很多时, 
                               各个分区的票数分散., 不容易找到一个满足条件的分区, 没有活动分区意味着整个集群不可用
                            2. 由于固定票数是不变的,所以不适用集群中有动态节点加入的场景
                            
                    b. 通过 Keep Majority 处理网络分区
                            Keep Majority 做法是不设置固定票数, 采用分区节点数 > 总节点数/2, 如果有 2 个, 则取分区内节点 ID 最小的,
                       它适用动态节点加入的场景, 同样的不适用分区数多,且分区内节点少,比较分散.
                       
                    c. 通过设置仲裁机制处理网络分区
                            将第三方的节点或则组件作为仲裁者, 分区内要选举出各自 leader, 分别与仲裁者通信, 其中通信内容有这个分区内的各个节点
                       信息, 由仲裁者来决定用哪个分区. 
                            缺点: 当某个分区无法 ping 通仲裁者, 但这个分区节点最多. 或则仲裁者单点故障了
                            
                    d. 基于共享资源的方式处理网络分区
                            类似与分布式锁的机制,  哪个子集群获得共享资源的锁, 就保留该子集群. 但是如果该集群发生故障, 导致锁没有释放,
                       其他的集群也无法拿到锁.
                            其适用的场景是获取锁的节点可靠性要有保证.
                       
           
                            
```
 
### 分布式存储- 三要素
```shell
    1. 概念
            (1) 分布式存储的三要素是 数据生产者/消费者, 数据索引, 数据存储.
            (2) 存储的数据特征被划分为三类：结构化数据、半结构化数据和非结构化数据
                        a. 结构化数据: 结构化数据通常是指关系模型数据, 其特征是数据关联较大、格式固定.火车票信息比如起点站、终点站、
                                     车次、票价等, 就是一种结构化数据. 结构化数据采用分布式关系数据库进行存储和查询
                                     
                        b. 半结构化数据: 是指非关系模型的, 有基本固定结构模式的数据, 其特征是数据之间关系比较简单. 比如 HTML 文档,
                                       使用标签书写内容. 半结构化数据大多可以采用 key-value, 对应与分布式键值系统
                                       
                        c. 非结构化数据: 是指没有固定模式的数据, 其特征是数据之间关联不大. 比如文本数据就是一种非结构化数据,这种数据可以
                                       存储到文档中, 通过 ElasticSearch（一个分布式全文搜索引擎）等进行检索, 存储对应于分布式存储系统
                                       
            
                                       
    2. 数据索引
            (1) 针对能够快速找到存储的数据, 引入了数据分片技术(分布式存储系统按照一定的规则将数据存储到相对应的存储节点), 
                可以通过数据范围，采用哈希映射、一致性哈希环等对数据划分
    
    
    3.为了分布式系统的可靠性,进行了数据复制
    
    4. 数据存储
            (1) 存储系统
                   a. 分布式数据库, 通过表格来存储结构化数据，方便查找。常用的分布式数据库有 MySQL Sharding
                   b. 分布式键值系统, 通过键值对(key-value)来存储半结构化数据.常用的分布式键值系统有 Redis、
                                     Memcache 等, 可用作缓存系统
                   c. 分布式存储系统, 通过文件、块、对象等来存储非结构化数据. 常见的分布式存储系统有 Ceph、GFS、HDFS、Swift 
```
 
### 分布式存储 - 数据索引
```shell
    1. 数据分布设计原则
            (1) 数据均匀维度
                    a. 考虑到存储的数据尽量均衡, 避免让某一个或某几个节点存储压力过大，而其他节点却几乎没什么数
                    b. 考虑到访问的数据所有在节点要均衡, 避免出现某一个或某几个节点的访问量很大,但其他节点却无人问津的情况
                    
            (2) 数据稳定性维度
                    考虑当通过某个规则将数据分配好加点后, 当某一节点发送故障, 最好不要出现其他节点里的数据也要进行大规模的数据再分配.
                比如, 现有 100G 数据, 刚开始有 4 个同类型节点（节点 1~4）,每个节点存储 25G 数据, 现在节点 2 故障了, 导致剩余每个节点
                需要存储 100G/3 数据, 那么根据规则原先数据需要重新分配节点, 尽量要保持只有节点 2 的数据进行分配, 其他节点数据尽量不进行
                分配.
                
            (3) 节点异构性维度
                    不同存储节点的硬件配置可能差别很大. 比如, 有的节点硬件配置很高, 可以存储大量数据, 也可以承受更多的请求,
                    有的节点硬件配置就不怎么样, 存储的数据量不能过多, 用户访问也不能过多.
                    
    2. 数据分布方法
            (1) 哈希
                    a. 通过一步计算直接得到相应的存储节点, 其中需要确定哈希函数
                    b. 优点: 只要哈希函数设置得当, 就可以保证数据均匀性.
                    c. 缺点: 稳定性差, 随着数据量的增加, 三个节点的容量无法再满足存储需求了，需要再添加一个节点。
                             哈希函数变成了 id%4 , 原先存储在那三个节点的数据需要重新计算, 然后存入相应节点, 即需要大规模的数据迁移,
                             显然会降低稳定性
                    d. 场景: 适用于同类型节点且节点数量比较固定, 例如 Redis
                    
            (2) 一致性哈希
                    a. 需要两步才可以找到相应的存储节点, 即在存储数据进行哈希前, 先对存储节点预先进行哈希(可以根据存储节点 ip 进行哈希),
                        这两个都映射到一个首尾相连的哈希环, 然后存储数据进行哈希计算, 顺时针找到第一个存储节点.
                       
                    b. 优点: 弥补哈希函数的稳定性问题, 当节点加入或退出时, 仅影响该节点在哈希环上顺时针相邻的后继节点. 如当 Node2 
                            发生故障需要移除时, 由于 Node3 是 Node2 顺时针方向的后继节点, 本应存储到 Node2 的数据就会
                            存储到 Node3 中，其他节点不会受到影响, 因此不会发生大规模的数据迁移
                            
                    c. 场景: 适合同类型节点、节点规模会发生变化, 例如 Cassandra
                    
                    d. 缺点: 虽然稳定性提高了,又会引发数据的均匀性, 节点发生故障,会对后继节点负载造成压力.
                    
            (3) 带有限负载的一致性哈希
                    a. 给每个存储节点设置了一个存储上限值来控制存储节点添加或移除造成的数据不均匀。当数据按照一致性哈希算法找到
                       相应的存储节点时，要先判断该存储节点是否达到了存储上限；如果已经达到了上限，则需要继续按顺时针查找满足要求的
                       存储节点.
                       
                    b.  场景: 适合同类型节点、节点规模会发生变化
                    
            哈希、一致性哈希、带有限负载的一致性哈希，都没有考虑节点异构性的问题
            
            (4) 带虚拟节点的一致性哈希
                    a. 考虑到存储节点的异构性, 根据每个节点的性能, 为每个节点划分不同数量的虚拟节点, 将这些虚拟节点映射到哈希环中,
                       然后再按照一致性哈希算法进行数据映射和存储.
                       例如, Node1 性能最差, Node2 性能一般, Node3 性能最好. 以 Node1 的性能作为参考基准, Node2 是 Node1 的 2 倍,
                            Node3 是 Node1 的 3 倍. Node1 对应一个虚拟节点 Node1_1, Node2 对应 2 个虚拟节点 Node2_1 和
                            Node2_2, Node3 对应 3 个虚拟节点 Node3_1、Node3_2 和 Node3_3.
                            虚拟节点 Node1_1、Node2_1、Node2_2、Node3_1、Node3_2、Node3_3 的 哈希值, 分别为 100, 200, 300,
                            400, 500, 600
                    
                    b. 场景: 适合异构节点、节点规模会发生变化, 例如,  Memcached 缓存系统
                    
    3. 扩展
            (1) 数据分区和数据分片的区别
                    数据分片是根据数据维度进行划分的, 将一个数据集合按照一定的方式划分到多个数据子集上,不同的数据子集存在不同的存储块上,
                    而这些存储块可以在不同的节点上, 也可以在同一个节点上.
                    
                    数据分区, 则是数据存储块的维度进行划分, 不同的分区归属于不同的节点
            
```
 
### 分布式存储 - 数据复制
 
```shell
    1. 概念
            (1) 只有主备数据库中的数据保持一致时, 才可实现主备的替换, 要保存主从数据库一致就得使用数据复制功能.
            (2) 根据 CPA 原理, 需要在可用性和一致性做出选择, 则对于数据复制方式有三种
                    第一种: 注重一致性. 同步复制技术
                    第二种: 注重可用性, 异步复制技术
                    第三种: 介于两者之间, 半同步复制技术
                    
    2. 同步复制技术
            (1) 当用户请求更新数据时, 主数据库必须要同步到备数据库之后才可给用户返回,如果主数据库没有同步到备数据库,
                用户的更新操作会一直阻塞, 确保了强一致性, 牺牲可用性. MySQL 集群支持的全复制模式就采用同步复制技术
                
            (2) 写/更新操作必须有主数据库进行操作, 同步更新到从数据库, 即使用户将写请求发送到从节点, 从节点也会将该请求转发给
                主节点
                
            (3) 场景
                    适用与分布式数据库一主一备场景, 因为要从主同步到从数据库,不太适用与多从数据库.
                    或则是对数据一致性有严格要求的场合, 比如金融, 交易的场景.
                    
    3. 异步复制技术
            (1) 异步复制技术是当用户请求更新数据时, 主数据库处理完请求后可直接给用户响应, 而
                不必等待备数据库完成同步,备数据库会异步进行数据的同步,用户的更新操作不会因为备数据库未完成数据同步而导致阻塞.
                其保证了系统的可用性,而忽视了数据的一致性.
                
            (2) 分布式数据库主备模式场景下, 若对数据一致性要求不高，也可以采用异步复制方法. MySQL 集群默认的数据复制模式采用的是
                异步复制技术
                
            (3) MySQL 默认的复制模式(异步复制技术)
                    第一步: 主数据库完成写操作后, 可直接给用户回复执行成功, 将写操作写入 binary log 中，
                            binary log 中记录着主数据库执行的所有更新操作, 以便备数据库获取.
                    第二步: 备数据库启动一个 IO 线程专门读取主数据库中 binary log 中的内容然后写入自己 relay log 文件中.
                    第三步: 备数据库启动一个 SQL 线程会定时检查 relay log 里的内容，如发现有新内容则会立即在备数据库中执行,
                           从而实现数据的一致
                           
            (4) 场景
                    异步复制技术主要应用在用户请求响应要求高的场景, 例如电商
                    
            (5) 应用
                    a. MySQL 的默认数据复制技术
                    b. Redis 复制技术
                    
    4. 半同步复制技术
            (1) 半同步复制技术: 用户发出写请求后, 主数据库会执行写操作, 并给备数据库发送同步请求, 但主数据库不用等待所有备数据库回复
                              数据同步成功便可响应用户, 也就是说主数据库可以等待一部分备数据库同步完成后响应用户写操作执行成功.
                              
            (2) 对于一部分从数据库的响应, 可以分为 2 种方式
                    a. 第一种方式, 偏于可用性, 主数据库只要收到其中一个从数据库成功响应, 就返回给用户.  MySQL 集群的一主多从下的
                                  半同步复制,就是采用这个方式.
                    b. 第二种方式, 偏于一致性, 主数据库收到超过一半从数据库成功响应, 就返回给用户. Raft 算法, ZooKeeper 集群的
                                 复制技术则采用的这种方式.在 ZooKeeper 集群中, 写请求必须由 Leader 节点进行处理, 
                                 每次写请求 Leader 会征求其他Follower 的同意, 只有当多数节点同意后写操作才可成功
                                 
                                 
            (3) 在半同步复制技术中, 一半都采用 Raft 算法(第二种方式), 对于那些没有回复的从节点而言, 解决数据一致性的问题,采用
                Leader 节点上会比较与自己数据不一致的 Follower 节点的信息，找到两者最后达成一致的地方, 然后强制将这个地方之后的数据
                复制到该 Follower 节点上. 
                例如: Leader 节点将每一次数据操作看作一条记录，并对这条记录标记一个 index, 用于索引. Leader 节点会为每个 Follower 
                     节点维护一个记录状态变量 nextIndex, 即下一个记录的索引位置( nextIndex 的值为 Leader 节点当前存储数据记录
                     的下一个 Index 值). Leader 节点会将 nextIndex 发送给 Follower 节点，若 Follower
                     节点发现与本节点的 nextIndex 不一致, 则告知 Leader 节点不一致, Leader 节点将
                     nextIndex 减 1, 重复上述过程，直到与 Follower 节点的 nextIndex 相等位置，即找到了两者最后达成一致的地方,
                     之后就将这个地方之后的数据拷贝到 Follows 后.
            
```
 
### 分布式存储 - 缓存技术
````shell
    1. 概念
    2. Redis 分布缓存原理
            (1) Redis(Remote Dictionary Server, 远程字典服务器), 是以 key-value 形式存在内存中, Redis 集群是一个典型的去中心
                化结构, 每个节点都负责一部分数据的存储, 每个节点进行主备设计提高 Redis 的可靠性.
                
                a. Redis 有 3 个主要特性, 支持多数据结构, 支持持久化, 支持主备同步
                b. Redis 的集群结构是每个节点负责一部分哈希槽, 且每个节点可以设计主备模式
                c .同步方式是采用异步方式, 再加上完整同步, 部分重同步.
                
            (2)  支持多数据结构
                    value 可以支持 List, Set, Hash 等复杂的数据结构
                    
            (3) 支持持久化
                    a. RDB(Redis DataBase), 快照方式, 定时将内存中的数据备份到磁盘中, 形成一个快照. 缺点是可能会造成数据丢失, 
                       因为是定时备份, 当节点发生故障时, 在内存中的数据还没有来得及保存到磁盘中的部分会丢失.
                       
                    b. AOF(Append Only File), 弥补 RDB 数据不一致问题, 就是记录下 Redis 中所有更新操作.
                       根据将操作记录到磁盘触发时机,可以分为 3 种, 第一种 AOF_FSYNC_NO(不会自动触发写操作的同步) , 
                       第二种 AOF_FSYNC_EVERYSEC(每隔一秒将写操作同步到磁盘), 
                       第三种 AOF_FSYNC_ALWAYS(每次发生写操作会立即同步到磁盘) 
                       
                       Redis 默认采用的是 AOF_FSYNC_EVERYSEC
                       
            (4) 支持主备同步
                    a. 平常的主数据库的更新
                            Redis 中通过 min-replicas-to-write 和 min-replicas-max-lag 来保证数据的一致性, 例如,  
                       min-replicas-to-write=3、min-replicas-max-lag=10, 表示当至少有 3 个从数据库连接主数据库的延迟时间小于
                       10s 时, 主数据库才可以执行写操作。延迟时间从最后一次收到备数据库的心跳开始计算, 通常备数据库每秒发送一次心跳
                       
                    b. 完整同步(备数据库刚启动时的数据同步)
                            第一步: 当备服务器启动时, 会向主服务器发送 SYNC 命令
                            第二步: 主服务器收到命令后会生成 RDB(快照)文件, 并记录从生成 RDB 文件起新执行的写操作.
                            第三步: RDB 生成后会发送给备服务器, 备服务器通过 RDB 文件进行数据同步更新
                            第四步: 更新完成后, 主服务器再将新记录的写操作发送给备服务器, 备服务器执行完这些新记
                                   录的写操作, 便与主服务器的数据保持一致
                                   
                    c. 部分重同步
                            背景: 因网络故障导致主备数据库断开连接, 待网络恢复后的备数据库的数据同步
                            方法: 主备数据库会共同维护一个复制偏移量, 在主数据库中保存了对从数据库发送的偏移量, 而从数据库也会保存
                                  自己已经同步的偏移量, 
                                  例如, 主数据库的复制偏移量为 5000 时, 向备数据库发送了 100 个字节的数据, 发送结束后复制偏移量为
                                       5100, 此时主备数据库连接断开, 备数据库没有接收到这 100 个字节的数据，等到备数据库重新
                                       与主数据库连接上之后, 会给主数据库发送 PSYNC 命令, 并带上自己的复制偏移量
                                       5000, 主数据库接收到信息后, 根据接收到的复制偏移量, 将 5000 之后的数据发给备数据库, 
                                       从而完成数据的部分重同步
                                       
    3. Memcached 分布式缓存原理
            (1) 与 Redis 存储分布不同, Memcached 集群采用的是带虚拟节点的一致性哈希算法
            (2) 对于数据结构的支持, 仅支持简单的 key-value(v 为 string), 不支持 list, hash.
            (3) 不支持持久化.
            (4) 自身不支持主备, 可以通过 Repcached 实现了 Memcached 的复制功能
                   
````
 
## 分布式高可靠
### 分布式高可靠 - 负载均衡
```shell
    1. 概念
            (1) 负载均衡方式
                    a. 请求负载均衡, 将用户的请求均衡地分发到不同的服务器处理
                    b. 数据负载均衡, 将用户更新的数据分发到不同的存储服务器
                    
    2. 请求负载均衡的策略
            (1) 轮询策略
                    a. 顺序轮询策略
                    b. 加权轮询策略, 普通的加权轮询策略, 如果优先级高的服务器很快就能处理掉请求, 则存在当部分请求到来时, 大多数请求
                       都集中在优先级较高的服务器上.
                       
                       nginx 是改进的加权轮询策略, 原理是每次请求时都要重新计算每个服务器的有效权值, 如果优先级最高有好几个, 则顺序选择
                       第一个.
                       
                       weight：在配置文件中, 服务器节点权重, 固定的值(基数)
                       effective_weight: 服务器的有效权重, 初始化时为 weight, 顺着这个节点服务异常, 会导致 effective_weight 变小
                       current_weigh: 初始化为 0, 之后每一次请求时都要计算 current_weigh = current_weigh + effective_weight
                       
                       例如: 各服务器的优先级是{4，1，1}
                            第一次请求: 计算每个服务器节点的 current_weigh(current_weigh = current_weigh + effective_weight),
                                       即各个服务器中 current_weigh 值为 {4, 1, 1}, total 4 + 1 + 1 = 6, 选出  
                                       current_weight 值最大的服务节点(即服务器 1 处理), 那么服务器 1 的 current_weigh 需要减去
                                       total, 即(4 - 6 = -2)
                                       
                            第二次请求: 计算每个服务器节点的 current_weigh, 为 {2, 2, 2}, total 为 6, 还是选取节点 1, 
                                       服务器 1 的 current_weigh 需要减去 total, 即(2 - 6 = -4)
                                       
                    优点: 实现简单, 加权轮询策略还考虑到服务器的异构性, 让性能更好的服务器具有更高的优先级, 从而可以处理更多的请求
                    缺点: 轮询策略每次请求目的地不确定, 不适用有状态的场景. 同时这个只考虑请求数的均衡性, 也不适用处理请求开销不同的场景
                    适用场景: 适用于用户请求所需资源比较接近
                    
            (2) 随机策略
                    a. 采用随机函数, 将请求尽可能平均分布在不同服务器, 缺点和场景和轮询策略差不多.
                    
    3. 数据负载均衡的策略
            (1) 采用哈希和一致性哈希策略.
            (2) 优点: 希函数设置合理, 负载也很均衡, 相同 key 的请求会落在同一个服务节点上, 适合有状态请求的场景. 带虚拟节点的一致性哈希
                      策略还可以解决服务器节点异构问题.
                      
            (3) 缺点: 当某一节点故障时, 如果是普通的哈希策略, 则会导致大量数据迁移. 也没有考虑请求开销不同造成不均衡问题.
```
 
### 分布式高可靠 - 流量控制
```shell
    1. 分布式流量控制是控制每个服务器接收的请求数, 以保证服务器来得及处理这些请求, 尽可能保证用户请求在服务器上持续得被处理, 
       而不是在服务器上阻塞等待被执行.
       
    2. 流量控制策略
            (1) 消息队列, 消除波峰
            (2) 漏桶策略
                    a. 概念
                            宽进严出的策略, 无论用户请求有多少, 请求速率有多大, 经过漏桶出去的速率始终是固定的, 服务器通过从漏桶出来
                       的请求可以稳定处理, 同时漏桶因为容量的限制放不下更多的请求时, 就会选择丢弃部分请求
                       
                    b. 优点
                            无论请求速率多大, 输出都是稳定的流量.
                    c. 缺点
                            对于突发性短暂大流量, 服务器还是和正常流量一样的处理, 导致请求处理不及时,严重会丢失请求.
                            
                    d. 场景
                            适用于间隔性突发流量且流量不用立马处理的, 在突发性大流量到达时,正常流出速率外的流量暂时缓存在漏桶中,
                       等待流量小的时候进行处理
                       
                            阿里的流量控制框架 Sentinel 匀速排队限流策略, 用的就是漏桶算法
                            
            (3) 令牌桶策略
                    a. 概念
                            一个有着上限固定令牌桶, 一面是以固定速率向桶中放入令牌, 一面是请求来时, 向桶中拿令牌, 只有拿到令牌的
                      请求才能被处理, 否则只能等待令牌. 这个桶中固定上限是服务器处理的上限.
                      
                    b. 场景
                            适用于有突发特性的流量, 且流量需要即时处理的. 因为当有突发大流量时, 只要令牌桶里有足够多的令牌,
                            请求就会被迅速执行
                            
                            
                            Google 开源工具包 Guava 提供的限流工具类 RateLimiter, 就是基于令牌桶算法.
                            
                            
    3. Sentinel 流量控制工作原理
            (1) 通过监控应用的并发线程数(从线程池中取出的并发线程数)来进行流量控制, 因为请求数多了,会导致取出并发的线程数也多, 
                当线程数达到一定阈值时, 直接数据请求.
                 
            (2) 通过监控 QPS 来进行流量控制
                    a. 直接拒绝, 适用场景是 对系统处理能力确切已知的情况下.
                    b. 预热
                            当系统长期处于一个较低的 QPS , 突然请求流量激增, 这时不会让系统立马处理这么多请求, 而是缓慢增加处理速度,
                       使其 QPS 缓慢增加达到一个规定的上限.类似于一个特殊的令牌桶, 令牌提供的速率会缓慢增加, 适用于具有突发特性的流量,
                       且流量可以即时处理的场景 
                                           
                    c. 匀速排队
                            就是漏桶策略. 漏桶具体的程序实现方法: 系统会设定一个时间间隔 T, 假设最大排队时长设置为 6T, 如果新到的
                        请求预期被处理超过 6T, 则拒接这个请求. 上次请求通过的时刻为 t1, 当新的请求在 t2 时刻到来的话, 根据是否需要
                        排队, 
                            如果不需要排队
                                当 t2 - t1 的值大于或等于时间间隔 T, 请求可以通过；
                                当 t2 - t1 的值小于 T 时，需要等待,直到 t2 - t1 的值达到时间间隔 T 时(为达到一个稳定的处理速率),
                                才可以让请求通过
                                
                            如果需要排队, 需要计算该新请求的预期通过时间。比如，有 3 个请求在排队, 则该新请求预期通过时间为 t1+4T,
                            因为需要等到在该请求前面的请求都通过后该请求才可通过，且两个请求通过的时间间隔必须达到 T 才可以. 当等待时间
                            超过 6T, 则拒接请求
                            
    4. 扩展
            (1) 流量控制和拥塞控制区别
                    如果是分布式领域, 流量控制是表示对业务的请求进行限制. 而拥塞控制则是网络数据传输, 是通过慢启动,拥塞避免.
    
```
 
### 分布式高可用 - 故障隔离
```shell
    1. 概念
            (1) 设计故障隔离策略是在系统设计时考虑的, 是提前对可能出现的故障进行预防, 使得正出现故障时进行隔离.
            
    2. 故障隔离策略
            (1) 以系统功能模块为粒度进行隔离, 将系统划分为不同的功能或则服务模块, 当一个功能故障不会影响到其他的功能. 根据功能模块由线程
                执行还是进程执行, 通常划分为线程级隔离和进程级隔离
                
            (2) 通过资源隔离来实现, 根据资源所属粒度, 通常包括容器隔离(即进程隔离), 虚拟机隔离, 服务器隔离和机房隔离
            
            
    3. 功能模块隔离
            (1) 线程级隔离
                    a. 一个进程又会多个不同的线程池, 每个线程池处理不同的业务请求, 当某个线程池发生故障后, 不会影响另外的线程池.
                    b. 适用场景: 适用单体应用, 即单进程多线程的应用
                    
            (2) 进程级隔离
                    a. 如果业务系统逐渐复杂, 则需要将各个功能拆分到不同进程, 这些进程可以分布在同一个服务器上,也可以分布在不同服务器上.
                       如果是分布在同一个服务器上, 则进程间的通信可以用管道, 消息队列, 共享内存. 如果是分布在不同的服务器上, 则通信
                       通过 IPC, 发布/订阅, 消息队列.
                       
    4. 资源隔离
            (1) 概念
                    将每部分资源分配给一个模块, 各个模块就不会争抢资源.
                     
            (2) 容器隔离        
                    一个微服务对应一个容器, 通过容器进行故障隔离, 容器是一个特殊的进程, 是可以进行资源限制的, 
                每个容器占用的资源(比如 CPU, 内存)都有一个上限, 限定该容器处理的能力, 这样容器不会无限制占用所有主机的资源. 
                2 个容器间也相互不影响.
                
            (3) 虚拟机级别的隔离
                    一台物理机可以安装多个虚拟机, 每个虚拟机都会分配一定的资源, 进行资源隔离
                    
    5. 用户级别隔离
            (1) 概念
                    将不同用户分开, 当系统出现故障时, 只影响部分用户, 而不是全体用户
            (2) 用户级别故障隔离策略
                    a. 数据分片, 将不同用户的数据存储到不同的数据库, 一个数据库只存储各自部分用户的信息. 当某个数据库出现故障时,
                                仅影响该故障数据库存储的用户, 而不会影响全部用户
                                
                    b. 负载均衡
                            当处理请求的某个服务器出现故障时, 只影响该故障服务器负责的用户请求, 而不会影响其他服务器负责的用户请求
```
 
### 分布式高可用 - 故障恢复
```shell
    1. 故障类型
            (1) 节点故障, 大致分为 2 类, 一类是硬件故障(比如机器硬盘损坏等), 另外一类是软件故障, 主要体现在软件层面, 例如请求过多,
                         超过服务的负载.
                         
            (2) 网络故障, 有路由器故障, DNS 故障, 网线断裂.
            
    2. 故障检测
            (1) 固定心跳检测策略
            (2) φ值故障检测(历史心跳消息预测故障), 统计以往的到达的间隔, 计算出样本的平均值 u, 和方差, 随着间隔不断更新, 其平均值和方差
                也会不断更新. 再通过平均值和方差,和当前的时间戳,计算出超时的概率. φ值故障检测策略可以根据历史心跳信息动态预测下一次心跳是
                否超时, 并可以通过设置阈值来自由调控误判的可能性. 实际应用: Akka 集群的故障检测, 采用 φ 值故障检测策略
                
            总结: 在网络状况确定且比较稳定的场景下, 采用固定心跳检测策略,因为其可以根据网络状况与业务场景自主设定合适的 k 和 T 值,
                 简单有效；
                 当网络状况有所变化, 且变化有规律的场景, 则可以使用φ值故障检测策略
    3. 故障处理
            (1) 指当故障产生时, 想办法让故障恢复, 即让系统能够正常工作
            (2) 单节点故障处理, 采用主备策略, 当主节点发生故障时, 集群通过选举使得备节点升为主节点, 对于用户来讲是无感知的.
            (3) 网络故障问题, 就是 C, P, A 的选择, 对于高可用性要求严格的系统, 请求需要及时的响应, 需要保 AP 弃 C .
                对于数据一致性有严格要求的系统, 比如银行、金融系统等场景, 就需要采用保 CP 弃 A 的策略. 网络故障恢复问题也可以看作
                数据复制的问题(处理方式有同步复制, 半同步赋值, 异步复制)
```