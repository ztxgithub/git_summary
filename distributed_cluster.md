# 分布式集群

## 集群

- 两大特性

``` shell

    1.可扩展性: 集群的性能是由多个相同服务实体构成的,新的服务实体可以动态地加入到集群,从而增强集群的性能
    2.高可用性: 集群通过服务实体冗余使客户端免于轻易遇到out of service的警告.即如果一台服务器崩溃了,可以切换到另外一台
                服务器,使得客户端的功能请求可以正常.
                	
```

- 两大能力

``` shell

    为了具有可扩展性和高可用性特点,集群的必须具备以下两大能力：
    
    1.负载均衡: 把任务比较均衡地分布到集群环境下的服务实体
    2.错误恢复: 由于某种原因,执行某个任务的资源出现故障,另一服务实体中执行同一任务的资源接着完成任务.
               这种由于一个实体中的资源不能工作，另一个实体中的资源透明的继续完成任务的过程叫错误恢复.
                	
```

- 两大技术

``` shell

   1.集群地址:客户端通过访问集群的集群地址获取集群内部各服务实体的功能.具有单一集群地址(也叫单一影像)是集群的一个基本特征.
             维护集群地址的设置被称为负载均衡器.负载均衡器内部负责管理各个服务实体的加入和退出,
             外部负责集群地址向内部服务实体地址的转换.
             
   2. 内部通信 : 为了能协同工作、实现负载均衡和错误恢复,集群各实体间必须时常通信,比如负载均衡器对服务实体心跳测试信息,
                服务实体间任务执行上下文信息的通信.
                
   具有同一个集群地址使得客户端能访问集群提供的计算服务,一个集群地址下隐藏了各个服务实体的内部地址,
   使得客户要求的计算服务能在各个服务实体之间分布.内部通信是集群能正常运转的基础,它使得集群具有均衡负载和错误恢复的能力.
                	
```

## 负载均衡

``` shell

     1.LB(Load Balance负载均衡器)
            LB负责分发设备的 MQTT连接与消息到 EMQ 集群,LB 提高 EMQ 集群可用性、实现负载平衡以及动态扩容.
            负载均衡器可以将来自多个公网地址的访问流量分发到多台主机上,并支持自动检测并隔离不可用的主机,
            从而提高业务的服务能力和可用性.你还可以随时通过添加或删减主机来调整你的服务能力,而且这些操作不会影响业务的正常访问.
            负载均衡器支持HTTP/HTTPS/TCP 三种监听模式,并支持透明代理,可以让后端主机不做任何更改就可以直接获取客户端真实IP.
            负载均衡器还支持灵活配置多种转发策略,实现高级的自定义转发控制功能.
            
            (1)均衡方式
                轮询：依据后端服务器的权重,将请求轮流发送给后端服务器,常用于短连接服务，例如 HTTP 等服务。
                
                最少连接：优先将请求发给拥有最少连接数的后端服务器,常用于长连接服务，例如数据库连接等服务。
                
                源地址：将请求的源地址进行hash运算,并结合后端的服务器的权重派发请求至某匹配的服务器,
                        这可以使得同一个客户端IP的请求始终被派发至某特定的服务器.该方式适合负载均衡无cookie功能的TCP协议.
                        
            (2)会话保持
            
                ip_hash实现tcp会话保持
                          
                会话保持可以将来自同一个客户端的请求始终发给同一个后端服务器,是通过 cookie 的方式来实现的.
                    植入cookie：由负载均衡器向客户端植入 cookie，这时你需要指定 cookie 的过期时间,不指定默认为不过期。
                    改写cookie：cookie 由你的后端业务来植入和管理,负载均衡器会通过改写该 cookie 的值来实现会话保持,
                                改写cookie对后端服务是透明的,不会影响后端服务的正常运行；
                                这时你需要指定需要改写的 cookie 名称.
                            
            (3)健康检查
                开启健康检测后,负载均衡器会根据你的配置定期检查后端服务的运行状态,当某个后端服务出现异常时,会自动隔离该后端服务,
                并将请求转发给其他健康的后端服务,实现高可用性。
            
                健康检查方式：
            
                    TCP：通过向后端服务器发送 TCP 包来检测后端服务
            
                    HTTP：通过向后端服务器发送HTTP请求来检测后端服务,你可以指定需要检测的URI.
                          负载均衡器会通过HTTP返回值是否为200来判断服务是否正常.
            
                健康检查选项：
            
                    检查间隔：连续两次健康检查之间的时间间隔，单位为秒，范围为 2 - 60s
            
                    超时时间：等待健康检查请求返回的超时时间，检查超时将会被判定为一次检查失败，单位为秒，范围为 5 - 300s
            
                    不健康阈值：多少次连续检查失败之后，可以将后端服务屏蔽，范围为 2 - 10次
            
                    健康阈值：多少次连续检查成功之后，可以将后端服务恢复，范围为 2 - 10次
                    
            (4)后端服务器权重
                当均衡方式为 “轮询” 时,你可以通过设置后端服务器的权重来让负载均衡器进行权重转发.权重的范围为 1 - 100,
                数值越大权重越高.
                
     2.四层、七层负载均衡
        四层就是基于IP+端口的负载均衡,七层就是基于URL等应用层信息的负载均衡,基于MAC地址的二层负载均衡和基于IP地址的三层负载均衡
        所谓的四到七层负载均衡,就是在对后台的服务器进行负载均衡时,依据四层的信息或七层的信息来决定怎么样转发流量.
        
        比如四层的负载均衡,就是通过发布三层的IP地址,然后加四层的端口号,来决定哪些流量需要做负载均衡,
        对需要处理的流量进行NAT处理(报文中目标IP地址进行修改(改为后端服务器IP)),转发至后台服务器,
        并记录下这个TCP或者UDP的流量是由哪台服务器处理的,后续这个连接的所有流量都同样转发到同一台服务器处理.
        此种Load Balance不理解应用协议（如HTTP/FTP/MySQL等等）.
        例子：LVS,F5
            Client->转发(修改报头目标地址+修改源地址根据需求)->server
        
        七层的负载均衡，就是在四层的基础上（没有四层是绝对不可能有七层的）,再考虑应用层的特征,
        比如同一个Web服务器的负载均衡,除了根据IP+80端口辨别是否需要处理的流量,还可根据七层的URL、浏览器类别、语言来决定
        是否要进行负载均衡.如果你的Web服务器分成两组,一组是中文语言的,一组是英文语言的,
        那么七层负载均衡就可以当用户来访问你的域名时,自动辨别用户语言,然后选择对应的语言服务器组进行负载均衡处理.
        Load Balancer能理解应用协议.例子： haproxy,MySQL Proxy.
        
        七层应用负载的好处,是使得整个网络更智能化.例如访问一个网站的用户流量,可以通过七层的方式,
        将对图片类的请求转发到特定的图片服务器并可以使用缓存技术,将对文字类的请求可以转发到特定的文字服务器并可以使用压缩技术.
        从技术原理上，这种方式可以对客户端的请求和服务器的响应进行任意意义上的修改,极大的提升了应用系统在网络层的灵活性.
        Nginx或者Apache上部署的功能可以前移到负载均衡设备上,例如客户请求中的Header重写,服务器响应中的关键字过滤或者内容插入等
        功能。
        另外一个常常被提到功能就是安全性.网络中最常见的SYN Flood攻击，即黑客控制众多源客户端,
        使用虚假IP地址对同一目标发送SYN攻击,通常这种攻击会大量发送SYN报文,耗尽服务器上的相关资源,
        以达到Denial of Service(DoS)的目的。从技术原理上也可以看出,四层模式下这些SYN攻击都会被转发到后端的服务器上；
        而七层模式下这些SYN攻击自然在负载均衡设备上就截止，不会影响后台服务器的正常运营.
        另外负载均衡设备可以在七层层面设定多种策略，过滤特定报文,例如SQL Injection等应用层面的特定攻击手段,
        从应用层面进一步提高系统整体安全.
        现在的7层负载均衡，主要还是着重于应用HTTP协议,所以其应用范围主要是众多的网站或者内部信息平台等基于B/S开发的系统。
         4层负载均衡则对应其他TCP应用，例如基于C/S开发的ERP等系统.
         
     3.负载均衡软件
        一种是通过硬件来进行,常见的硬件有比较昂贵的F5和Array等商用的负载均衡器,它的优点就是有专业的维护团队来对这些服务进行维护
        、缺点就是花销太大，所以对于规模较小的网络服务来说暂时还没有需要使用；
        另外一种就是类似于Nginx/LVS/HAProxy的基于 Linux的开源免费的负载均衡软件，这些都是通过软件级别来实现,
        所以费用非常低廉。
        
        目前关于网站架构一般比较合理流行的架构方案：Web前端采用Nginx/HAProxy+ Keepalived作负载均衡器；
        后端采用 MySQL数据库一主多从和读写分离,采用LVS+Keepalived的架构.
        
        Nginx的优点是：
            (1).工作在网络的7层之上,可以针对http应用做一些分流的策略,比如针对域名、目录结构,
            它的正则规则比HAProxy更为强大和灵活,这也是它目前广泛流行的主要原因之一,Nginx单凭这点可利用的场合就远多于LVS了.
            (2)Nginx对网络稳定性的依赖非常小，理论上能ping通就就能进行负载功能，这个也是它的优势之一；
                相反LVS对网络稳定性依赖比较大.
            (3)可以承担高负载压力且稳定,在硬件不差的情况下一般能支撑几万次的并发量,负载度比LVS相对小些.
            (4)Nginx可以通过端口检测到服务器内部的故障,比如根据服务器处理网页返回的状态码、超时等等,
                并且会把返回错误的请求重新提交到另一个节点,不过其中缺点就是不支持url来检测.
                比如用户正在上传一个文件，而处理该上传的节点刚好在上传过程中出现故障,Nginx会把上传切到另一台服务器重新处理，
                而LVS就直接断掉了
                
        Nginx的缺点是:
            (1)Nginx仅能支持http、https和Email协议,这样就在适用范围上面小些,这个是它的缺点.
            (2)对后端服务器的健康检查,只支持通过端口来检测,不支持通过url来检测.不支持Session的直接保持,但能通过ip_hash来解决.
            
        LVS的优点是:
            (1) 抗负载能力强、是工作在网络4层之上仅作分发之用,没有流量的产生,这个特点也决定了它在负载均衡软件里的性能最强的,
                对内存和cpu资源消耗比较低
            (2) 工作稳定,因为其本身抗负载能力很强,自身有完整的双机热备方案，如LVS+Keepalived.
            (3) 无流量,LVS只分发请求,而流量并不从它本身出去,这点保证了均衡器IO的性能不会受到大流量的影响
            (4) 应用范围比较广，因为LVS工作在4层，所以它几乎可以对所有应用做负载均衡，包括http、数据库、在线聊天室等等
            
        LVS的缺点是：
            (1) 软件本身不支持正则表达式处理,不能做动静分离；而现在许多网站在这方面都有较强的需求,
                这个是Nginx/HAProxy+Keepalived的优势所在.
            (2) 如果是网站应用比较庞大的话,LVS/DR+Keepalived实施起来就比较复杂了,
                特别后面有 Windows Server的机器的话,如果实施及配置还有维护过程就比较复杂了,
                Nginx/HAProxy+Keepalived就简单多了.
                
        HAProxy的特点是：
            (1) HAProxy的优点能够补充Nginx的一些缺点,比如支持Session的保持,Cookie的引导；
                同时支持通过获取指定的url来检测后端服务器的状态
            (2) HAProxy支持TCP协议的负载均衡转发,可以对MySQL读进行负载均衡,对后端的MySQL节点进行检测和负载均衡，
                大家可以用LVS+Keepalived对MySQL主从做负载均衡
                
            (3)HAProxy负载均衡策略非常多，HAProxy的负载均衡算法现在具体有如下8种：
               ① roundrobin，表示简单的轮询，这个不多说，这个是负载均衡基本都具备的；
               ② static-rr，表示根据权重，建议关注；
               ③ leastconn，表示最少连接者先处理，建议关注；
               ④ source，表示根据请求源IP，这个跟Nginx的IP_hash机制类似，我们用其作为解决session问题的一种方法，建议关注；
               ⑤ ri，表示根据请求的URI；
               ⑥ rl_param，表示根据请求的URl参数’balance url_param’ requires an URL parameter name；
               ⑦ hdr(name)，表示根据HTTP请求头来锁定每一次HTTP请求；
               ⑧ rdp-cookie(name)，表示根据据cookie(name)来锁定并哈希每一次TCP请求.
               
               
        对网络负载均衡的使用是随着网站规模的提升使用不同的技术：
            第一阶段：利用Nginx或HAProxy进行单点的负载均衡,这一阶段服务器规模刚脱离开单服务器、单数据库的模式,
                    需要一定的负载均衡,但是仍然规模较小没有专业的维护团队来进行维护,也没有需要进行大规模的网站部署.
                    这样利用Nginx或HAproxy就是第一选择,此时这些东西上手快,配置容易,在七层之上利用HTTP协议就可以.
                    
            第二阶段：随着网络服务进一步扩大,这时单点的Nginx已经不能满足,这时使用LVS或者商用Array就是首要选择,
                    Nginx此时就作为LVS或者Array的节点来使用,具体LVS或Array的是选择是根据公司规模和预算来选择,
                    Array的应用交付功能非常强大,但是一般来说这阶段相关人才跟不上业务的提升,
                    所以购买商业负载均衡已经成为了必经之路.
                    
            第三阶段：这时网络服务已经成为主流产品,此时随着公司知名度也进一步扩展，相关人才的能力以及数量也随之提升,
                    这时无论从开发适合自身产品的定制,以及降低成本来讲开源的LVS,已经成为首选，这时LVS会成为主流.
                    
        最终形成比较理想的基本架构为：Array/LVS — Nginx/Haproxy — Squid/Varnish — AppServer.
                     	
```

## 反向代理

``` shell

   1.反向代理：工作在反向代理模式下的负载均衡设备,则是外网用户通过代理设备访问内网
                
   2.正向代理:普通的代理设备是内网用户通过代理设备出外网进行访问,
                	
```

## 分布式系统工程实践

```shell
    1.分布式系统的本质困难是 partial failure, 分布式系统不是放大的单机系统,根本原因是单机没有部分故障(partial failure)
      对应单机我们能轻易判断某个进程或则硬件是否还在正常运行.当在分布式系统中,我们无法得知另外一台机器是网络故障还是程序故障. 
      而且在分布式中事件的顺序性也没法保证,比如一台机器先发出消息,但由于网络原因不一定先收到
      
    2. 基于客户端视角的负载均衡策略
            这里站在客户端的角度思考,判断各个服务器端的负载情况可以根据该客户端向各个服务端请求时未收到响应的个数,即
            未收到响应的个数越少则代表该服务器负载小.
            
            实现思路: 客户端维护一个队列,该队列保存该客户端向各个服务器请求消息时未响应的个数,当要发送请求时,从
                     上次调用服务端的下一个位置进行遍历判断,选择负载最轻的服务器(即请求消息时未响应的个数最少)
                     
            在多客户端的情况下,防止潮涌(多个客户端一起向相同的服务端请求),刚开始访问服务器下标可以根据 ip 地址, MAC地址,
            时间等产生随机数种子
            
    3. 分布式系统的可靠性
            高可用的关键不在于不停机,而在于随时能够重启服务
            (1) 不要使用跨进程的 mutex 或则 semaphore, 也不要使用共享内存，因为进程意外终止，无法清理资源，
            　　　特别是无法解锁．也不要使用父子进程共享文件描述符通信(pipe)
            
            (2) 优雅的重启
                    A. 先主动停止一个服务进程的心跳：
                            对于短链接，关闭 listen port, 不会有新的请求到达
                            对于长连接，客户端会主动 failover 到备用的服务器端
                            
                    B. 等一段时间，待服务程序处理完剩下的请求，直到没有活动的请求
                    C. kill 并重启服务
                    D. 检测新进程的服务是否重启
                    E. 依次重启服务端的
                    
            (3) TCP 连接作为分布式系统进程间通信的唯一方式．原因是任何一方进程意外退出，对方都能及时的收到通知(操作系统会及时得
            　　关闭进程的 TCP socket)
            (4) TCP keepalive 不能代替应用层的心跳，原因是心跳除了判断网络是否正常，还可以判断该程序是否正常运行．
            　　　通常是服务器向客户端发送心跳．
                　心跳协议的关键点:
                        1. 尽量在工作线程中发送，不要另起一个线程．防止工作线程阻塞或则死锁，但心跳还是正常发送．
                        2. 与业务消息采用同一条连接，不要新开 TCP 连接．
                        
            (5) 分布式系统的进程标识
                    A. 进程标识是用来唯一标识一个程序的"一次运行"，同一份代码多次运行其对应的进程标识不同．
                    B. 用 ip:port 来标识进程，如果该进程服务为无状态的(没有上下文关系)，则没有问题．如果
                    　　该进程服务是有状态关系的，客户端无法区分该 ip:port　是新的进程还是原来的进程，导致某些
                    　　业务不正常．
                    C. 用 host:pid 标识进程，需考虑 pid 可能会轮回
                    D. 正确表示进程标识(ip:port:start_time:pid), 而　ip:port:start_time　不能保证唯一性，
                    　　如果程序短时间内重启，start_time可能会一样．
                    　　有了唯一的 gpid, 那么全局唯一的消息 id 可以用计数器递增的值和 gpid 组合起来
                    
            (6) 构建易于维护的分布式程序
                    易于维护即管理方便，日常劳动负担小．长期运行的且与其他机器继续通信交互的进程应该要提供管理接口，
                    可以查看进程的状态．一种具体的做法是在程序中内置一个 HTTP 服务器，能够查看进程的健康状态及当前
                    负载．
                    采用　HTTP　的原因是
                          A. 本身是 TCP Server ,可以安全的方便防止重复启动(bind() 失败退出)
                          B. 不必使用特定的客户端程序，用普通的 web 浏览器就可以
                          C. HTTP 是文本协议，在异常条件下可以使用 wget/curl 来获取内容
                          D. 可以借助 url 来划分请求的业务
                          
            (7) 为系统后续升级做准备
                    升级之前一定要做好 rollback 计划，做好回退准备
                    A. 要定义跨语言的可扩展的消息格式
                            1.可扩展的消息格式要求：内容中避免协议版本号，否则代码中会有一堆难以维护的 case-switch
                            2.可扩展的消息格式的内容，如果是文本格式数据，可以采用 JSON 或则 XML. 如果是二进制格式
                            　则可以采用 Google Protocol Buffer.
                            
            (8) 分布式自动化回归测试
                    A. 单元测试
                            1. 单元测试主要测试一个函数，一个 class 
                            2. 单一测试的缺点
                                    I. 阻碍大型重构．单元测试属于白盒测试，需要编写测试代码来调用被测代码(被测的函数)
                                    　　所以测试代码与被测代码紧耦合．在添加新功能的时候，我们常会重构已有的代码，
                                    　　在保持原来功能的情况下重构的代码能够用到其他的地方，这样一来原先单元测试就
                                    　　通不过．这样又得修改单元测试，耗时
                                    II. 为了方便测试而实施依赖注入，破坏代码的完整性．单看一块代码不知道到它是干嘛的，
                                    　　　它依赖的对象不知道在哪儿创建的．
                                    
                   　B. 一种自动化的回归测试方案　test harness , 这是一个独立的进程，模拟与被测程序的交互的所有程序．
                            1. test harness 优点
                                    (1) 完全从外部观察被测程序，没有对被测程序进行侵入，被测程序代码不需要像
                                    　　　单元测试一样考虑接口
                                    (2) 允许被测程序做大的重构，以优化内部代码结构，只要外部表现的形式不变(接口报文不变)
                                    (3) 有了一套完整的 test harness,甚至可以换种语言重写被测程序，测试用例依然可用．
                                    
                            2.　实现机制
                                    (1) test harness 能发起或则多个连接，可能需要用到现成的 NIO 网络库
                                    (2) test harness 只需要表现跟它 mock 程序一样，不需要真正的实现其业务逻辑，
                                    　　　只需要给被测程序的信息一样就可以了．如果要 mock 比较复杂的逻辑，可以
                                    　　　用 "记录 + 回放"的方式，把预设的响应回放到被测程序中．
                            
            (9) 分布式系统的运维
                    运维包括部署(升级)可执行程序与配置文件，监控进程状态，管理服务进程(重启服务)，故障响应
                    1.等级一: 全手工操作(实验室水平)
                            部署: 编译后手工将可执行程序拷贝到各台机器上，配置文件也要手工修改放到对应的机器上
                            管理: 手工启动服务进程，并且显式指名配置文件的路劲，重启进程需要登陆到 host (机器)
                            　　　进行 restart
                            升级: 如果需要升级服务进程，需要手动登陆多台 hosts,并拷贝新的可执行程序，重启
                            配置: Web Server 配置文件里写了 Suduku Sover 的 ip:port, 如果要部署新的
                            　　　Suduku Sover，则需要在　Web Server 配置文件增加并重启 Web Server
                            
                    2. 等级二: 使用零散的自动化脚本和第三方组件
                            这一层级主要是公司刚起步，开发重心在实现新功能上，顾不上高效的运维，运维工作基本上由
                            开发人员兼职．
                                部署: 部署可执行程序用脚本实现,为了让 c++ 可执行程序拷贝到 host 就能运行，通常采用静态运行
                                　　　以避免 .so 版本不同造成故障．同时服务进程的配置文件需要放在 git(版本管理) 上，
                                　　　每个不同的服务进程实例都有自己的 branch, 每次修改都要入库．程序启动时用到的配置文件
                                    　都应该从 git check out , 不能手工修改(减少人为失误)
                                管理: 服务进程使用 daemon 方式管理，异常 crash 后自动重启
                                升级: 可执行程序也有一套版本管理，发布新版本时不能覆盖原先老的可执行文件
                                　　　现在运行的是 /v1.0.0/bin/solver
                                     需要发布的是 /v2.0.0/bin/solver
                                     原因是　(1) 已经把老的替换成新的，那么老的可执行程序不存在，在老的可执行程序运行一段时间
                                     　　　　　　　后会出现 bus error
                                            (2) 当出现 core dump ,需要与"产生该 core dump 的可执行程序" 配对
                                            
                                监控: 公司会使用开源的软件监控每台 host 的资源使用情况，
                                
                    3. 等级三: 自制机群管理系统，集中化配置
                            这一层级主要适合成熟的大公司
                            部署:只需要向 Master 发送一条指令， Master 会命令　Slave 从指定的地点 rsync 新的可执行
                            　　　程序到本地目录
                            进程监控和管理: Sudoku Solver 是由 Slave 节点 fork() 产生的，同时父子进程可以通过 pipe()
                                          进行消息的通信，　Slave 节点得知 Sudoku Solver 异常 crash 后会令其重启．
                                          Master 提供一个 Web 页面来We看本机群中各个服务程序是否正常．
                                          
                            升级: 如果要主动重启 Sudoku Solver ,可以向 Master 发送指令，不需要 ssh & kill,
                                 同时会保存每台 host 的服务进程启动时间．动态的获知配置情况，当一个系统新增
                                 Sudoku Solver　服务，可以通过 Web Server 管理接口向其发送增加 ip:port 命令．
                                 还有一种方法就是通过数据库，每个 Sudoku Solver 服务启动时往数据库表中 insert 
                                 自身的 ip:port, Web Server 配置里写了 select 语句,获取可利用的服务的 ip:port
                                 Web Server 通过数据库触发器及时的感知 Sudoku Solver ip address list 的变化.
                            
                            
                          
                    

```