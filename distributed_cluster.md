# 分布式集群

## 集群

- 两大特性

``` shell

    1.可扩展性: 集群的性能是由多个相同服务实体构成的,新的服务实体可以动态地加入到集群,从而增强集群的性能
    2.高可用性: 集群通过服务实体冗余使客户端免于轻易遇到out of service的警告.即如果一台服务器崩溃了,可以切换到另外一台
                服务器,使得客户端的功能请求可以正常.
                	
```

- 两大能力

``` shell

    为了具有可扩展性和高可用性特点,集群的必须具备以下两大能力：
    
    1.负载均衡: 把任务比较均衡地分布到集群环境下的服务实体
    2.错误恢复: 由于某种原因,执行某个任务的资源出现故障,另一服务实体中执行同一任务的资源接着完成任务.
               这种由于一个实体中的资源不能工作，另一个实体中的资源透明的继续完成任务的过程叫错误恢复.
                	
```

- 两大技术

``` shell

   1.集群地址:客户端通过访问集群的集群地址获取集群内部各服务实体的功能.具有单一集群地址(也叫单一影像)是集群的一个基本特征.
             维护集群地址的设置被称为负载均衡器.负载均衡器内部负责管理各个服务实体的加入和退出,
             外部负责集群地址向内部服务实体地址的转换.
             
   2. 内部通信 : 为了能协同工作、实现负载均衡和错误恢复,集群各实体间必须时常通信,比如负载均衡器对服务实体心跳测试信息,
                服务实体间任务执行上下文信息的通信.
                
   具有同一个集群地址使得客户端能访问集群提供的计算服务,一个集群地址下隐藏了各个服务实体的内部地址,
   使得客户要求的计算服务能在各个服务实体之间分布.内部通信是集群能正常运转的基础,它使得集群具有均衡负载和错误恢复的能力.
                	
```

## 负载均衡

``` shell

     1.LB(Load Balance负载均衡器)
            LB负责分发设备的 MQTT连接与消息到 EMQ 集群,LB 提高 EMQ 集群可用性、实现负载平衡以及动态扩容.
            负载均衡器可以将来自多个公网地址的访问流量分发到多台主机上,并支持自动检测并隔离不可用的主机,
            从而提高业务的服务能力和可用性.你还可以随时通过添加或删减主机来调整你的服务能力,而且这些操作不会影响业务的正常访问.
            负载均衡器支持HTTP/HTTPS/TCP 三种监听模式,并支持透明代理,可以让后端主机不做任何更改就可以直接获取客户端真实IP.
            负载均衡器还支持灵活配置多种转发策略,实现高级的自定义转发控制功能.
            
            (1)均衡方式
                轮询：依据后端服务器的权重,将请求轮流发送给后端服务器,常用于短连接服务，例如 HTTP 等服务。
                
                最少连接：优先将请求发给拥有最少连接数的后端服务器,常用于长连接服务，例如数据库连接等服务。
                
                源地址：将请求的源地址进行hash运算,并结合后端的服务器的权重派发请求至某匹配的服务器,
                        这可以使得同一个客户端IP的请求始终被派发至某特定的服务器.该方式适合负载均衡无cookie功能的TCP协议.
                        
            (2)会话保持
            
                ip_hash实现tcp会话保持
                          
                会话保持可以将来自同一个客户端的请求始终发给同一个后端服务器,是通过 cookie 的方式来实现的.
                    植入cookie：由负载均衡器向客户端植入 cookie，这时你需要指定 cookie 的过期时间,不指定默认为不过期。
                    改写cookie：cookie 由你的后端业务来植入和管理,负载均衡器会通过改写该 cookie 的值来实现会话保持,
                                改写cookie对后端服务是透明的,不会影响后端服务的正常运行；
                                这时你需要指定需要改写的 cookie 名称.
                            
            (3)健康检查
                开启健康检测后,负载均衡器会根据你的配置定期检查后端服务的运行状态,当某个后端服务出现异常时,会自动隔离该后端服务,
                并将请求转发给其他健康的后端服务,实现高可用性。
            
                健康检查方式：
            
                    TCP：通过向后端服务器发送 TCP 包来检测后端服务
            
                    HTTP：通过向后端服务器发送HTTP请求来检测后端服务,你可以指定需要检测的URI.
                          负载均衡器会通过HTTP返回值是否为200来判断服务是否正常.
            
                健康检查选项：
            
                    检查间隔：连续两次健康检查之间的时间间隔，单位为秒，范围为 2 - 60s
            
                    超时时间：等待健康检查请求返回的超时时间，检查超时将会被判定为一次检查失败，单位为秒，范围为 5 - 300s
            
                    不健康阈值：多少次连续检查失败之后，可以将后端服务屏蔽，范围为 2 - 10次
            
                    健康阈值：多少次连续检查成功之后，可以将后端服务恢复，范围为 2 - 10次
                    
            (4)后端服务器权重
                当均衡方式为 “轮询” 时,你可以通过设置后端服务器的权重来让负载均衡器进行权重转发.权重的范围为 1 - 100,
                数值越大权重越高.
                
     2.四层、七层负载均衡
        四层就是基于IP+端口的负载均衡,七层就是基于URL等应用层信息的负载均衡,基于MAC地址的二层负载均衡和基于IP地址的三层负载均衡
        所谓的四到七层负载均衡,就是在对后台的服务器进行负载均衡时,依据四层的信息或七层的信息来决定怎么样转发流量.
        
        比如四层的负载均衡,就是通过发布三层的IP地址,然后加四层的端口号,来决定哪些流量需要做负载均衡,
        对需要处理的流量进行NAT处理(报文中目标IP地址进行修改(改为后端服务器IP)),转发至后台服务器,
        并记录下这个TCP或者UDP的流量是由哪台服务器处理的,后续这个连接的所有流量都同样转发到同一台服务器处理.
        此种Load Balance不理解应用协议（如HTTP/FTP/MySQL等等）.
        例子：LVS,F5
            Client->转发(修改报头目标地址+修改源地址根据需求)->server
        
        七层的负载均衡，就是在四层的基础上（没有四层是绝对不可能有七层的）,再考虑应用层的特征,
        比如同一个Web服务器的负载均衡,除了根据IP+80端口辨别是否需要处理的流量,还可根据七层的URL、浏览器类别、语言来决定
        是否要进行负载均衡.如果你的Web服务器分成两组,一组是中文语言的,一组是英文语言的,
        那么七层负载均衡就可以当用户来访问你的域名时,自动辨别用户语言,然后选择对应的语言服务器组进行负载均衡处理.
        Load Balancer能理解应用协议.例子： haproxy,MySQL Proxy.
        
        七层应用负载的好处,是使得整个网络更智能化.例如访问一个网站的用户流量,可以通过七层的方式,
        将对图片类的请求转发到特定的图片服务器并可以使用缓存技术,将对文字类的请求可以转发到特定的文字服务器并可以使用压缩技术.
        从技术原理上，这种方式可以对客户端的请求和服务器的响应进行任意意义上的修改,极大的提升了应用系统在网络层的灵活性.
        Nginx或者Apache上部署的功能可以前移到负载均衡设备上,例如客户请求中的Header重写,服务器响应中的关键字过滤或者内容插入等
        功能。
        另外一个常常被提到功能就是安全性.网络中最常见的SYN Flood攻击，即黑客控制众多源客户端,
        使用虚假IP地址对同一目标发送SYN攻击,通常这种攻击会大量发送SYN报文,耗尽服务器上的相关资源,
        以达到Denial of Service(DoS)的目的。从技术原理上也可以看出,四层模式下这些SYN攻击都会被转发到后端的服务器上；
        而七层模式下这些SYN攻击自然在负载均衡设备上就截止，不会影响后台服务器的正常运营.
        另外负载均衡设备可以在七层层面设定多种策略，过滤特定报文,例如SQL Injection等应用层面的特定攻击手段,
        从应用层面进一步提高系统整体安全.
        现在的7层负载均衡，主要还是着重于应用HTTP协议,所以其应用范围主要是众多的网站或者内部信息平台等基于B/S开发的系统。
         4层负载均衡则对应其他TCP应用，例如基于C/S开发的ERP等系统.
         
     3.负载均衡软件
        一种是通过硬件来进行,常见的硬件有比较昂贵的F5和Array等商用的负载均衡器,它的优点就是有专业的维护团队来对这些服务进行维护
        、缺点就是花销太大，所以对于规模较小的网络服务来说暂时还没有需要使用；
        另外一种就是类似于Nginx/LVS/HAProxy的基于 Linux的开源免费的负载均衡软件，这些都是通过软件级别来实现,
        所以费用非常低廉。
        
        目前关于网站架构一般比较合理流行的架构方案：Web前端采用Nginx/HAProxy+ Keepalived作负载均衡器；
        后端采用 MySQL数据库一主多从和读写分离,采用LVS+Keepalived的架构.
        
        Nginx的优点是：
            (1).工作在网络的7层之上,可以针对http应用做一些分流的策略,比如针对域名、目录结构,
            它的正则规则比HAProxy更为强大和灵活,这也是它目前广泛流行的主要原因之一,Nginx单凭这点可利用的场合就远多于LVS了.
            (2)Nginx对网络稳定性的依赖非常小，理论上能ping通就就能进行负载功能，这个也是它的优势之一；
                相反LVS对网络稳定性依赖比较大.
            (3)可以承担高负载压力且稳定,在硬件不差的情况下一般能支撑几万次的并发量,负载度比LVS相对小些.
            (4)Nginx可以通过端口检测到服务器内部的故障,比如根据服务器处理网页返回的状态码、超时等等,
                并且会把返回错误的请求重新提交到另一个节点,不过其中缺点就是不支持url来检测.
                比如用户正在上传一个文件，而处理该上传的节点刚好在上传过程中出现故障,Nginx会把上传切到另一台服务器重新处理，
                而LVS就直接断掉了
                
        Nginx的缺点是:
            (1)Nginx仅能支持http、https和Email协议,这样就在适用范围上面小些,这个是它的缺点.
            (2)对后端服务器的健康检查,只支持通过端口来检测,不支持通过url来检测.不支持Session的直接保持,但能通过ip_hash来解决.
            
        LVS的优点是:
            (1) 抗负载能力强、是工作在网络4层之上仅作分发之用,没有流量的产生,这个特点也决定了它在负载均衡软件里的性能最强的,
                对内存和cpu资源消耗比较低
            (2) 工作稳定,因为其本身抗负载能力很强,自身有完整的双机热备方案，如LVS+Keepalived.
            (3) 无流量,LVS只分发请求,而流量并不从它本身出去,这点保证了均衡器IO的性能不会受到大流量的影响
            (4) 应用范围比较广，因为LVS工作在4层，所以它几乎可以对所有应用做负载均衡，包括http、数据库、在线聊天室等等
            
        LVS的缺点是：
            (1) 软件本身不支持正则表达式处理,不能做动静分离；而现在许多网站在这方面都有较强的需求,
                这个是Nginx/HAProxy+Keepalived的优势所在.
            (2) 如果是网站应用比较庞大的话,LVS/DR+Keepalived实施起来就比较复杂了,
                特别后面有 Windows Server的机器的话,如果实施及配置还有维护过程就比较复杂了,
                Nginx/HAProxy+Keepalived就简单多了.
                
        HAProxy的特点是：
            (1) HAProxy的优点能够补充Nginx的一些缺点,比如支持Session的保持,Cookie的引导；
                同时支持通过获取指定的url来检测后端服务器的状态
            (2) HAProxy支持TCP协议的负载均衡转发,可以对MySQL读进行负载均衡,对后端的MySQL节点进行检测和负载均衡，
                大家可以用LVS+Keepalived对MySQL主从做负载均衡
                
            (3)HAProxy负载均衡策略非常多，HAProxy的负载均衡算法现在具体有如下8种：
               ① roundrobin，表示简单的轮询，这个不多说，这个是负载均衡基本都具备的；
               ② static-rr，表示根据权重，建议关注；
               ③ leastconn，表示最少连接者先处理，建议关注；
               ④ source，表示根据请求源IP，这个跟Nginx的IP_hash机制类似，我们用其作为解决session问题的一种方法，建议关注；
               ⑤ ri，表示根据请求的URI；
               ⑥ rl_param，表示根据请求的URl参数’balance url_param’ requires an URL parameter name；
               ⑦ hdr(name)，表示根据HTTP请求头来锁定每一次HTTP请求；
               ⑧ rdp-cookie(name)，表示根据据cookie(name)来锁定并哈希每一次TCP请求.
               
               
        对网络负载均衡的使用是随着网站规模的提升使用不同的技术：
            第一阶段：利用Nginx或HAProxy进行单点的负载均衡,这一阶段服务器规模刚脱离开单服务器、单数据库的模式,
                    需要一定的负载均衡,但是仍然规模较小没有专业的维护团队来进行维护,也没有需要进行大规模的网站部署.
                    这样利用Nginx或HAproxy就是第一选择,此时这些东西上手快,配置容易,在七层之上利用HTTP协议就可以.
                    
            第二阶段：随着网络服务进一步扩大,这时单点的Nginx已经不能满足,这时使用LVS或者商用Array就是首要选择,
                    Nginx此时就作为LVS或者Array的节点来使用,具体LVS或Array的是选择是根据公司规模和预算来选择,
                    Array的应用交付功能非常强大,但是一般来说这阶段相关人才跟不上业务的提升,
                    所以购买商业负载均衡已经成为了必经之路.
                    
            第三阶段：这时网络服务已经成为主流产品,此时随着公司知名度也进一步扩展，相关人才的能力以及数量也随之提升,
                    这时无论从开发适合自身产品的定制,以及降低成本来讲开源的LVS,已经成为首选，这时LVS会成为主流.
                    
        最终形成比较理想的基本架构为：Array/LVS — Nginx/Haproxy — Squid/Varnish — AppServer.
                     	
```

## 反向代理

``` shell

   1.反向代理：工作在反向代理模式下的负载均衡设备,则是外网用户通过代理设备访问内网
                
   2.正向代理:普通的代理设备是内网用户通过代理设备出外网进行访问,
                	
```

## 分布式系统工程实践

```shell
    1.分布式系统的本质困难是 partial failure, 分布式系统不是放大的单机系统,根本原因是单机没有部分故障(partial failure)
      对应单机我们能轻易判断某个进程或则硬件是否还在正常运行.当在分布式系统中,我们无法得知另外一台机器是网络故障还是程序故障. 
      而且在分布式中事件的顺序性也没法保证,比如一台机器先发出消息,但由于网络原因不一定先收到
      
    2. 基于客户端视角的负载均衡策略
            这里站在客户端的角度思考,判断各个服务器端的负载情况可以根据该客户端向各个服务端请求时未收到响应的个数,即
            未收到响应的个数越少则代表该服务器负载小.
            
            实现思路: 客户端维护一个队列,该队列保存该客户端向各个服务器请求消息时未响应的个数,当要发送请求时,从
                     上次调用服务端的下一个位置进行遍历判断,选择负载最轻的服务器(即请求消息时未响应的个数最少)
                     
            在多客户端的情况下,防止潮涌(多个客户端一起向相同的服务端请求),刚开始访问服务器下标可以根据 ip 地址, MAC地址,
            时间等产生随机数种子
            
    3. 分布式系统的可靠性
            高可用的关键不在于不停机,而在于随时能够重启服务
            (1) 不要使用跨进程的 mutex 或则 semaphore, 也不要使用共享内存，因为进程意外终止，无法清理资源，
            　　　特别是无法解锁．也不要使用父子进程共享文件描述符通信(pipe)
            
            (2) 优雅的重启
                    A. 先主动停止一个服务进程的心跳：
                            对于短链接，关闭 listen port, 不会有新的请求到达
                            对于长连接，客户端会主动 failover 到备用的服务器端
                            
                    B. 等一段时间，待服务程序处理完剩下的请求，直到没有活动的请求
                    C. kill 并重启服务
                    D. 检测新进程的服务是否重启
                    E. 依次重启服务端的
                    
            (3) TCP 连接作为分布式系统进程间通信的唯一方式．原因是任何一方进程意外退出，对方都能及时的收到通知(操作系统会及时得
            　　关闭进程的 TCP socket)
            (4) TCP keepalive 不能代替应用层的心跳，原因是心跳除了判断网络是否正常，还可以判断该程序是否正常运行．
            　　　通常是服务器向客户端发送心跳．
                　心跳协议的关键点:
                        1. 尽量在工作线程中发送，不要另起一个线程．防止工作线程阻塞或则死锁，但心跳还是正常发送．
                        2. 与业务消息采用同一条连接，不要新开 TCP 连接．
                        
            (5) 分布式系统的进程标识
                    A. 进程标识是用来唯一标识一个程序的"一次运行"，同一份代码多次运行其对应的进程标识不同．
                    B. 用 ip:port 来标识进程，如果该进程服务为无状态的(没有上下文关系)，则没有问题．如果
                    　　该进程服务是有状态关系的，客户端无法区分该 ip:port　是新的进程还是原来的进程，导致某些
                    　　业务不正常．
                    C. 用 host:pid 标识进程，需考虑 pid 可能会轮回
                    D. 正确表示进程标识(ip:port:start_time:pid), 而　ip:port:start_time　不能保证唯一性，
                    　　如果程序短时间内重启，start_time可能会一样．
                    　　有了唯一的 gpid, 那么全局唯一的消息 id 可以用计数器递增的值和 gpid 组合起来
                    
            (6) 构建易于维护的分布式程序
                    易于维护即管理方便，日常劳动负担小．长期运行的且与其他机器继续通信交互的进程应该要提供管理接口，
                    可以查看进程的状态．一种具体的做法是在程序中内置一个 HTTP 服务器，能够查看进程的健康状态及当前
                    负载．
                    采用　HTTP　的原因是
                          A. 本身是 TCP Server ,可以安全的方便防止重复启动(bind() 失败退出)
                          B. 不必使用特定的客户端程序，用普通的 web 浏览器就可以
                          C. HTTP 是文本协议，在异常条件下可以使用 wget/curl 来获取内容
                          D. 可以借助 url 来划分请求的业务
                          
            (7) 为系统后续升级做准备
                    升级之前一定要做好 rollback 计划，做好回退准备
                    A. 要定义跨语言的可扩展的消息格式
                            1.可扩展的消息格式要求：内容中避免协议版本号，否则代码中会有一堆难以维护的 case-switch
                            2.可扩展的消息格式的内容，如果是文本格式数据，可以采用 JSON 或则 XML. 如果是二进制格式
                            　则可以采用 Google Protocol Buffer.
                            
            (8) 分布式自动化回归测试
                    A. 单元测试
                            1. 单元测试主要测试一个函数，一个 class 
                            2. 单一测试的缺点
                                    I. 阻碍大型重构．单元测试属于白盒测试，需要编写测试代码来调用被测代码(被测的函数)
                                    　　所以测试代码与被测代码紧耦合．在添加新功能的时候，我们常会重构已有的代码，
                                    　　在保持原来功能的情况下重构的代码能够用到其他的地方，这样一来原先单元测试就
                                    　　通不过．这样又得修改单元测试，耗时
                                    II. 为了方便测试而实施依赖注入，破坏代码的完整性．单看一块代码不知道到它是干嘛的，
                                    　　　它依赖的对象不知道在哪儿创建的．
                                    
                   　B. 一种自动化的回归测试方案　test harness , 这是一个独立的进程，模拟与被测程序的交互的所有程序．
                            1. test harness 优点
                                    (1) 完全从外部观察被测程序，没有对被测程序进行侵入，被测程序代码不需要像
                                    　　　单元测试一样考虑接口
                                    (2) 允许被测程序做大的重构，以优化内部代码结构，只要外部表现的形式不变(接口报文不变)
                                    (3) 有了一套完整的 test harness,甚至可以换种语言重写被测程序，测试用例依然可用．
                                    
                            2.　实现机制
                                    (1) test harness 能发起或则多个连接，可能需要用到现成的 NIO 网络库
                                    (2) test harness 只需要表现跟它 mock 程序一样，不需要真正的实现其业务逻辑，
                                    　　　只需要给被测程序的信息一样就可以了．如果要 mock 比较复杂的逻辑，可以
                                    　　　用 "记录 + 回放"的方式，把预设的响应回放到被测程序中．
                            
            (9) 分布式系统的运维
                    运维包括部署(升级)可执行程序与配置文件，监控进程状态，管理服务进程(重启服务)，故障响应
                    1.等级一: 全手工操作(实验室水平)
                            部署: 编译后手工将可执行程序拷贝到各台机器上，配置文件也要手工修改放到对应的机器上
                            管理: 手工启动服务进程，并且显式指名配置文件的路劲，重启进程需要登陆到 host (机器)
                            　　　进行 restart
                            升级: 如果需要升级服务进程，需要手动登陆多台 hosts,并拷贝新的可执行程序，重启
                            配置: Web Server 配置文件里写了 Suduku Sover 的 ip:port, 如果要部署新的
                            　　　Suduku Sover，则需要在　Web Server 配置文件增加并重启 Web Server
                            
                    2. 等级二: 使用零散的自动化脚本和第三方组件
                            这一层级主要是公司刚起步，开发重心在实现新功能上，顾不上高效的运维，运维工作基本上由
                            开发人员兼职．
                                部署: 部署可执行程序用脚本实现,为了让 c++ 可执行程序拷贝到 host 就能运行，通常采用静态运行
                                　　　以避免 .so 版本不同造成故障．同时服务进程的配置文件需要放在 git(版本管理) 上，
                                　　　每个不同的服务进程实例都有自己的 branch, 每次修改都要入库．程序启动时用到的配置文件
                                    　都应该从 git check out , 不能手工修改(减少人为失误)
                                管理: 服务进程使用 daemon 方式管理，异常 crash 后自动重启
                                升级: 可执行程序也有一套版本管理，发布新版本时不能覆盖原先老的可执行文件
                                　　　现在运行的是 /v1.0.0/bin/solver
                                     需要发布的是 /v2.0.0/bin/solver
                                     原因是　(1) 已经把老的替换成新的，那么老的可执行程序不存在，在老的可执行程序运行一段时间
                                     　　　　　　　后会出现 bus error
                                            (2) 当出现 core dump ,需要与"产生该 core dump 的可执行程序" 配对
                                            
                                监控: 公司会使用开源的软件监控每台 host 的资源使用情况，
                                
                    3. 等级三: 自制机群管理系统，集中化配置
                            这一层级主要适合成熟的大公司
                            部署:只需要向 Master 发送一条指令， Master 会命令　Slave 从指定的地点 rsync 新的可执行
                            　　　程序到本地目录
                            进程监控和管理: Sudoku Solver 是由 Slave 节点 fork() 产生的，同时父子进程可以通过 pipe()
                                          进行消息的通信，　Slave 节点得知 Sudoku Solver 异常 crash 后会令其重启．
                                          Master 提供一个 Web 页面来看本机群中各个服务程序是否正常．
                                          
                            升级: 如果要主动重启 Sudoku Solver ,可以向 Master 发送指令，不需要 ssh & kill,
                                 同时会保存每台 host 的服务进程启动时间．动态的获知配置情况，当一个系统新增
                                 Sudoku Solver　服务，可以通过 Web Server 管理接口向其发送增加 ip:port 命令．
                                 还有一种方法就是通过数据库，每个 Sudoku Solver 服务启动时往数据库表中 insert 
                                 自身的 ip:port, Web Server 配置里写了 select 语句,获取可利用的服务的 ip:port
                                 Web Server 通过数据库触发器及时的感知 Sudoku Solver ip address list 的变化.
```

## 分布式系统知识点

```shell
    1. SOA : Service Oriented Architecture ,面向服务架构, 
       SOA 架构是构造分布式计算应用程序的方法。它将 应用程序功能作为服务 发送给最终用户或者其他服务
```

## 分布式系统架构本质

```shell
    1.采用分布式系统而不用单机系统的原因
            (1) 增大系统容量．随着业务量越来越大，一台机器已经无法满足要求，需要垂直或则水平拆分系统业务，
            　　让其变为分布式系统. 其主要工作是大流量处理，对应的技术是通过集群技术把大规模并发请求的负载
            　　分散到不同的机器上．
            (2) 增加系统的可用性(高可用)，提高系统的高可用，则该架构中不能存在单点故障，则需要通过分布式架构来冗余系统．
                关键业务保护,提高后台服务的可用性，把故障隔离起来阻止多米诺骨牌效应（雪崩效应）。
                如果流量过大，需要对业务降级，以保护关键业务流转。
            (3) 模块化，开发效率高 
             
    2. 分布式系统的劣势
            (1) 系统架构的设计复杂(例如分布式事务)
            (2) 部署分布式系统复杂(而单机系统则比较简单)
            (3) 系统吞吐量会变大，但其响应的时间会变长(要考虑到分发处理)
            
    3. 分布式系统架构的难点在于系统设计，以及管理和运维,虽然解决了"单点”和“性能容量”的问题, 但会引发其他的问题，
    　　需要不断地用各式各样的技术和手段来解决这些问题．
    
    4. 面向服务的架构的阶段
            (1) 第一阶段: 单体架构，模块的耦合性高，模块功能之间的相互关联
            (2) 第二阶段: SOA 架构，需要一个标准的协议或是中间件来联动其它相关联的服务（如 ESB）
            　　　　　　　　服务间并不直接依赖，而是通过中间件的标准协议或是通讯框架相互依赖
            (3) 第三阶段: 微服务架构，耦合度低，每一个微服务都能独立完整地运行，
            　　　　　　　　它和传统 SOA 的差别在于，服务间的整合需要一个服务编排或是服务整合的引擎，
            　　　　　　　　这个编排和组织引擎可以是工作流引擎，也可以是网关。还需要辅助于像容器化调度这样的技术方式，如 Kubernetes
            
    5. 分布式系统的难点
            (1) 分布式系统的不标准问题
                    a. 软件和应用不标准。 不同的软件，不同的语言会出现不同的兼容性和不同的开发、测试、运维标准
                    b. 通讯协议不标准。不同的软件用不同的协议.
                    c. 数据格式不标准。不同的团队中相同的网络协议里也会出现不同的数据格式.
                    d. 开发和运维的过程和方法不标准。
                    
                    A. 一个好的配置管理，应该分成三层,底层(和操作系统相关)，中间层(和中间件相关)，最上层(和业务应用相关)
                       底层和中间层是不能让用户灵活修改的，只能让用户选择
                       
                    B. 规范数据通讯协议．作为一个协议，一定要有协议头和协议体。协议头定义最基本的协议数据，而协议体才是真正的业务数据。
                    　对于协议头，我们需要非常规范地让每一个使用这个协议的团队都使用一套标准的方式来定义，
                    　这样我们才容易对请求进行监控、调度和管理。
                    
            (2) 系统架构中服务依赖性问题
                    分布式架构下，服务是会有依赖的，于是一个服务依赖链上，某个服务挂掉了，会导致出现“多米诺骨牌”效应，会倒一片。
                    服务依赖具体带来了问题:
                        A. 服务依赖链中，出现“木桶短板效应”——整个 SLA(Service-Level Agreement服务级别协议) 由最差的那个服务所决定。
                        
                    这是服务治理的内容.服务治理不但需要我们定义出服务的关键程度，还需要我们定义或是描述出关键业务或服务调用的主要路径。
                    没有这些，我们将无法运维或是管理整个系统
                    
                    我们不但要拆分服务，还要为每个服务拆分相应的数据库．数据库方面也需要做相应的隔离，系统间不能读取对方的数据库，
                    只通过服务接口耦合，这也是微服务的要求
                    
            (3) 故障发生的概率更大(但产生的影响面小)
                    在分布式系统中，因为使用的机器和服务会非常多，所以，故障发生的频率会比传统的单体应用更大，但产生的影响面小
                    这时将引发运维团队在分布式系统下会非常忙，我们可以定义一些关键指标，同时在设计系统时考虑如何减轻故障。
                    如果无法避免，也要使用自动化的方式恢复故障，减少故障影响面。
                    
            (4) 多层架构的运维复杂度更大
                    系统分成四层,由下到上划分：
                        a. 基础层就是我们的机器、网络和存储设备等。
                        b. 平台层就是我们的中间件层，Tomcat、MySQL、Redis、Kafka 之类的软件。
                        c. 应用层就是我们的业务软件，比如，各种功能的服务。
                        d. 接入层就是接入用户请求的网关、负载均衡(nginx)或是 CDN、DNS 
                        
                    这逻辑架构的划分带来的影响是:
                        a. 任何一层的问题都会导致整体的问题；
                        b. 没有统一的视图和管理，导致运维被割裂开来，造成更大的复杂度。
                    
                    应对策略: 规范标准
```

## 分布式系统技术栈

```shell
    1. 提高架构的性能
            (1) 缓存系统(缓存分区，缓存更新，缓存命中)
                   加入缓存系统，可以有效地提高系统的访问能力。从前端的浏览器，到网络，再到后端的服务，底层的数据库、文件系统、
                   硬盘和 CPU，全都有缓存，这是提高快速访问能力最有效的手段。对于分布式系统下的缓存系统，需要的是一个缓存集群。
                   这其中需要一个 Proxy 来做缓存的分片和路由
            (2) 负载均衡系统(网关系统，负载均衡，服务路由，服务发现)
                    是做水平扩展的关键技术，用多台机器来共同分担一部分流量请求
            (3) 异步调用(消息队列，消息持久，异步事务)
                    异步系统主要通过消息队列来对请求做排队处理，把前端的请求的峰值给“削平”了，而后端通过自己能够处理的速度来处理请求。
                    这样可以增加系统的吞吐量，但是实时性就差很多了。还会引入消息丢失的问题，所以要对消息做持久化，
                    这会造成“有状态”的结点，从而增加了服务调度的难度。
            (4) 数据分区和数据镜像
                    数据分区是把数据按一定的方式分成多个区（比如通过地理位置），不同的数据区来分担不同区的流量。
                    这需要一个数据路由的中间件，会导致跨库的 Join 和跨库的事务非常复杂。而数据镜像是把一个数据库镜像成多份一样的数据，
                    这样就不需要数据路由的中间件了。你可以在任意结点上进行读写，内部会自行同步数据。然而，
                    数据镜像中最大的问题就是数据的一致性问题。对于一般公司来说，在初期，会使用读写分离的数据镜像方式，
                    而后期会采用分库分表的方式。
                    
    2. 提高系统稳定性
            (1) 服务拆分(服务调用，服务依赖，服务隔离)
                    主要有两个目的：一是为了隔离故障，二是为了重用服务模块。但服务拆分完之后，会引入服务调用间的依赖问题
            (2) 服务冗余(弹性伸缩，故障迁移，服务发现)
                    是为了去除单点故障，并可以支持服务的弹性伸缩，以及故障迁移。然而，对于一些有状态的服务来说，
                    冗余这些有状态的服务带来了更高的复杂性。弹性伸缩时，需要考虑数据是复制还是重新分片，
                    故障迁移的时候还要连同数据一起迁移
            (3) 限流降级(异步队列，降级控制，服务熔断)
                    当系统实在扛不住压力时，只能通过限流或者功能降级的方式来停掉一部分服务，或是拒绝一部分用户，
                    以确保整个架构不会挂掉。这些技术属于保护措施
            (4) 高可用架构(多租户系统，灾备多活，高可用服务)
                    从冗余架构的角度来保障可用性。比如，多租户隔离，灾备多活，或是数据一致性的集群。总之，就是为了不出单点故障。
            (5) 高可用运维(全栈监控，DevOps, 自动化运维)
                    指的是 DevOps 中的 CI（持续集成）/CD（持续部署）。一个良好的运维应该是一条很流畅的软件发布管线，
                    其中做了足够的自动化测试，还可以做相应的灰度发布，以及对线上系统的自动化控制。
                    这样，可以做到“计划内”或是“非计划内”的宕机事件的时长最短
                    
    3. 分布式系统的关键技术
            (1) 服务治理
                    服务拆分、服务调用、服务发现，服务依赖，服务的关键度定义……
                    服务治理的最大意义是需要把服务间的依赖关系、服务调用链，以及关键的服务给梳理出来，
                    并对这些服务进行性能和可用性方面的管理
            (2) 架构软件管理
                    服务之间有依赖，而且有兼容性问题，所以，整体服务所形成的架构需要有架构版本管理、整体架构的生命周期管理，
                    以及对服务的编排、聚合、事务处理等服务调度功能。
            (3) DevOps
                    分布式系统可以更为快速地更新服务，但是对于服务的测试和部署都会是挑战。所以，还需要 DevOps 的全流程，
                    其中包括环境构建、持续集成(CI)、持续部署(CD)等
            (4) 自动化运维
                    有了 DevOps 后，我们就可以对服务进行自动伸缩、故障迁移、配置管理、状态管理等一系列的自动化运维技术了。
            (5) 资源调度管理
                    应用层的自动化运维需要基础层的调度支持，也就是云计算 IaaS 层的计算、存储、网络等资源调度、隔离和管理
            (6) 整体架构监控
                    如果没有一个好的监控系统，那么自动化运维和资源调度管理只可能成为一个泡影，因为监控系统是你的眼睛。
                    没有眼睛，没有数据，就无法进行高效的运维(没有一个好的监控系统，我们将无法进行自动化运维和资源调度)
                    所以说，监控是非常重要的部分。
                    这里的监控需要对三层系统（应用层、中间件层、基础层）进行监控
            (7) 流量控制
                    负载均衡、服务路由、熔断、降级、限流等和流量相关的调度都会在这里，包括灰度发布之类的功能也在这里
                    
            根据以上那么多的技术，学习成本，投入的人力物力较大，可以通过 Docker 以及其衍生出来的 Kubernetes 之类的软件或解决方案，
            大大地降低了做上面很多事情的门槛．Docker 把软件和其运行的环境打成一个包，然后比较轻量级地启动和运行。
            在运行过程中，软件变成了服务　可能会改变　现有的环境。但是没关系，当你重新启动一个 Docker 的时候，
            环境又会变成初始化状态。

    4. 分布式系统的“纲”
            由于分布式系统所需要的技术太复杂，所以我们不能着眼于每个细节，而需要掌握关键技术．
                (1) 全栈系统监控(应用整体监控)
                (2) 服务 / 资源调度；
                (3) 流量调度；
                (4) 状态 / 数据调度；
                (5) 开发和运维的自动化。 是需要把前四项都做到了，才有可能实现的
```

### 分布式系统的关键技术一：全栈系统监控

```shell
    1. 监控系统需要具备的功能:
            (1) 全栈监控；
            (2) 关联分析；
            (3) 跨系统调用的串联；
            (4) 实时报警和自动处置；
            (5) 系统性能分析。
            
    2. 全栈监控(对基础层，中间层，应用层进行监控)
            (1) 基础层：监控主机和底层资源。比如：CPU、内存、网络吞吐、硬盘 I/O、硬盘使用等。
            (2) 中间层：就是中间件层的监控。比如：Nginx(网关)、Redis(缓存服务)、ActiveMQ、
            　　　　　　Kafka(消息队列)、MySQL、Tomcat(Java容器) 等。
            (3) 应用层：监控应用层的使用。比如：HTTP 访问的吞吐量、响应时间、返回码，
                       调用链路分析，性能瓶颈，还包括用户端的监控。
                       
            将这些监控数据进行标准化:
                a. 日志数据结构化；
                b. 监控数据格式标准化；
                c. 统一的监控平台；
                d. 统一的日志分析。
                
    3. 现实中很多监控系统存在的问题
            (1) 监控数据都是隔离开来的。
                    因为公司分工的问题，开发、应用运维、系统运维，各管各的，所以很多公司的监控系统也是各是各的，完全串不起来。
            (2) 监控的数据项太多。
                    有些公司的运维团队把监控的数据项多做为一个亮点到处讲，比如监控指标达到 5 万多个。这不合理，
                    因为信息太多等于没有信息，抓不住重点的监控才会做成这个样子
                    
    4. 一个好的监控系统具备的特点
            (1) 容量管理
                     提供一个全局的系统其在运行过程中数据的展示，可以让工程师团队知道是否需要增加机器或者其它资源。
            (2) 性能管理分析
                    可以通过查看大盘，找到系统瓶颈，并有针对性地优化系统和相应代码。
            (3) 定位问题
                    可以快速地暴露并找到问题的发生点，帮助技术人员诊断问题。故障发生不可怕，可怕的是故障的恢复时间过长，
                    需要快速的定位问题
                    
    5. 如何做出一个好的监控系统
            (1) 服务调用链跟踪
                    这个监控系统应该从对外的 API 开始，然后将后台的实际服务给关联起来，然后这个服务的依赖服务给关联起来，
                    直到最后一个服务（如 MySQL 或 Redis），这样就可以把整个系统的服务全部都串连起来了。
                    这个事情的最佳实践是 Google Dapper 系统，其对应于开源的实现是 Zipkin。
                    对于 Java 类的服务，我们可以使用字节码技术进行字节码注入，做到代码无侵入式。
            (2) 服务调用时长分布
                    使用 Zipkin, 可以看到一个服务调用链上的时间分布(各个服务的时间分布)，这样有助于我们知道最耗时的服务是什么
            (3) 服务的 TOP N 视图
                    所谓 TOP N 视图就是一个系统请求的排名情况。 排名的方法：a）按调用量排名，b) 按请求最耗时排名
            (4) 数据库操作关联
                    对于 Java 应用，我们可以很方便地通过 JavaAgent 字节码注入技术拿到 JDBC 执行数据库操作的执行时间。
                    对此，我们可以和相关的请求对应起来。
            (5) 服务资源跟踪
                    
    6.好的监控系统的作用
            有了以上数据，同时把数据关联好。这样，我们才可能很快地定位故障，进而才能进行自动化调度，
            比如：
                一旦发现某个服务过慢是因为 CPU 使用过多，我们就可以做弹性伸缩。
                一旦发现某个服务过慢是因为 MySQL 出现了一个慢查询，我们就无法在应用层上做弹性伸缩，只能做流量限制，或是降级操作了。
  
```

### 分布式系统的关键技术二：服务调度

```shell
    1.服务治理的关键技术
            (1) 服务关键程度
            (2) 服务依赖关系。
            (3) 服务发现。
            (4) 整个架构的版本管理。
            (5) 服务应用生命周期全管理
            
    2. 服务关键程度和服务的依赖关系
            (1) 服务关键程度要我们梳理和定义服务的重要程度。这不是使用技术可以完成的，这需要对业务的有较深理解，
                才能定义出架构中各个服务的重要程度
            (2) 梳理出服务间的依赖关系，服务间的依赖是一件很易碎的事。依赖越多越复杂，我们的系统就越易碎
            　　一个服务的问题很容易出现一条链上的问题。因此，传统的 SOA 希望通过 ESB 来解决服务间的依赖关系，
            　　这也是为什么微服务中希望服务间是没有依赖的，而让上层或是前端业务来整合这些个后台服务．
            　　服务依赖最优解的上限是微服务，而服务依赖的下限是千万不要有依赖环．如果系统架构中有服务依赖环，
            　　那么表明你的架构设计是错误的。循环依赖有很多的副作用，最大的问题是这是一种极强的耦合，会导致服务部署相当复杂和难解，
            　　而且会导致无穷尽的递归故障和一些你意想不到的的问题。
            
               服务的依赖关系是可以通过技术的手段来发现的，这其中，Zipkin是一个很不错的服务调用跟踪系统，
               它是通过 Google Dapper这篇论文来实现的。这个工具可以帮你梳理服务的依赖关系，以及了解各个服务的性能。
               
           　(3) 在梳理完服务的重要程度和服务依赖关系之后，我们就相当于知道了整个架构的全局。
           
    3. 服务状态和生命周期的管理
            (1) 在梳理完服务的重要程度和服务依赖关系之后,有了对架构的大致了解，这时需要［服务发现］这个中间件，来知道在这个动态的
            　　架构中，当前所处的状态
                    a. 整个架构中有多少种服务？
                    b. 这些服务的版本是什么样的？
                    c. 每个服务的实例数有多少个，它们的状态是什么样的?
                    d. 每个服务的状态是什么样的？是在部署中，运行中，故障中，升级中，还是在回滚中，伸缩中，或者是在下线中……
            (2) 服务状态
                    a. Provision，代表在供应一个新的服务；
                    b. Ready，表示启动成功了；
                    c. Run，表示通过了服务健康检查；
                    d. Update，表示在升级中；
                    e. Rollback，表示在回滚中。
                    f. Scale，表示正在伸缩中（可以有 Scale-in 和 Scale-out 两种）。
                    g. Destroy，表示在销毁中。
                    h. Failed，表示失败状态。
                    
            (3) 整个架构的版本管理
                    除了各个项目的版本管理之外(也就是各个服务的版本)，还需要在上面再盖一层版本管理(架构版本)，
                    用来控制其中各个服务的版本兼容。比如，A 服务的 1.2 版本只能和 B 服务的 2.2 版本一起工作，
                    A 服务的上个版本 1.1 只能和 B 服务的 2.0 一起工作。这就是版本兼容性
                    如果我们要回滚一个服务的版本，就可以把与之有版本依赖的服务也一起回滚掉。
                    
                    这时版本管理需要一个服务清单：
                        a. 服务的软件版本；
                        b. 服务的运行环境——环境变量、CPU、内存、可以运行的结点、文件系统等；
                        c. 服务运行的最大最小实例数。
                        
                    每一次对这个清单的变更都需要被记录下来，算是一个架构的版本管理。
                    而我们上面所说的那个集群控制系统需要能够解读并执行这个清单中的变更，以操作和管理整个集群中的相关变更
                    
```