# Linux 流量控制

- 什么是流量控制

``` shell

     流量控制就是在路由器上通过一系列队列，对数据包进行排序以控制它们的发送顺序，并通过一系列策略控制收到的和发送的数据包是否应该被丢弃，
     同时还要对数据包的发送速率进行控制.
     QoS（Quality of Service，服务质量）通常也被当做是一种实施流量控制的方法.
```

[参考资料](http://www.wy182000.com/wordpress/wp-content/uploads/2013/04/overview.html)

- 令牌桶

``` shell

    系统以一定的速率产生令牌，每个数据包（或一个字节）对应一个令牌，只有当令牌充足的时候数据包才能出队.
    打个比方来说,在游乐园里有很多游客在排队等待乘坐过山车。让我们假设过山车按照固定的时间到来。游客必须等待一列过山车到来后才能乘坐。
    在这里，过山车上的位置就相当于令牌，而游客就相等于数据包。这就是网络速率限制，或者称为shaping。在单位时间内，
    只能有固定数量的游客乘上过山车（出队）。继续这个过山车的比喻，假设某个时刻没有游客排队，而车站里有很多车子，这时候来了一大波游客，
    那么这些游客的大多数人（甚至是全部）都能立刻乘上过山车（因为此时车站里有很多空车）。车站里所能停放的过山车数量就是令牌桶中“桶”的大小。
    桶中积攒起来的令牌能在突然出现大量数据包时全部使用出去。
    让我们完成这个比喻。游乐园里的过山车以恒定速率到来，如果没有游客排队的话，就停放在等待区里，直到等待区里停满了车子。
    在令牌桶模型中，令牌以恒定的速率填充到桶中，直到桶满了为止。使用令牌桶模型进行流量整形能应付诸如HTTP应用之类会产生突发数据传输的情形.
    
    一个装满的令牌桶能在一定时间内应付任何类型的流量。比较恒定的网络流量适合使用较小的令牌桶。
    经常有突发数据传输的网络则比较适合使用大的令牌桶，除非流量整形的目的是为了限制突发数据传输(这种情况适合emqttd服务器).
    
    令牌桶模型也被应用于TBF（一种classless qdiscs）和HTB（一种classful qdiscs）中。
```

## 流量控制中的概念

``` shell
    1.整形(shaping)
        整形就是流量控制，把数据包的发送速率控制在一个固定的水平以下,由于整形通过延迟数据包的发送来控制数据包发送速率，
        故整形机制是非工作保存的。“非工作保存”可以理解为：系统必须进行一些操作来延迟数据包的发送。反过来说，
        一种非工作保存的队列是可以进行流量整形的，而工作保存的队列（参考 PRIO）不能进行流量整形，因为工作保存队列无法延迟发送数据包.
        在流量整形中，通常会使用令牌桶机制来实现整形.
        
    2.调度(scheduling)
        一个调度器会对将要发送的数据包顺序进行排列或重排。最常见的调度器是FIFO（先入先出队列）。由于数据包必须按顺序出队，
        因此队列实际上就是一个调度器。对于不同的网络环境，我们可以使用不同的调度器。
        一个公平队列算法（参考SFQ）能防止一个客户端或一个数据流占用过多的带宽。
        一个循环算法（参考WRR）可以让各个客户端或数据流都有平等的使用网络的机会。还有一些更复杂的算法可用于防止骨干网流量过载
        
    3.分类(classifying)
        分类器能把不同类型的网络流量划分到不同的队列中去。通常我们只对上行流量进行分类。在路由器接收、路由并转发一个数据包的时候，
        网络设备可以以几种不同的方式给数据包进行分类。其中一种方式是标记数据包。标记数据包的操作可以在一个网络中由管理员进行设置，
        也有可能在数据包经过每一跳时发生。
        Linux允许数据包通过一系列的流量控制结构，期间允许用户使用决策器对数据包进行分类（参考第 4.3 节 “过滤器”和第 4.5 节 “决策器”）.
        
    4.策略(policing)
        决策器能计算并限制某个特定队列的流量。在流量控制中使用决策器来控制流量是非常简单的。决策器通常会应用于网络边界上，
        某个节点使用的流量不会超过分配给它的流量。当网络流量在预设值以下时，决策器什么都不会做。但当网络流量超过预设值时，
        决策器就开始发挥作用，它能将流量速率控制在一个固定的水平之下。最不近人情的操作是即使在数据包能够继续分类的情况下依旧直接将其丢弃。
        决策器只会区别对待两种情况，分别是入队数据包速率高于或低于预定速率。当入队速率低于预设值时，决策器就会允许数据包入队。
        当入队速率高于预设值时，决策器就执行其他操作（丢包或重新分类）。虽然决策器内部使用令牌桶来计算速率，
        但它并不像shaping(整形)那样会延迟数据包的发送
        
    5.标记(marking)
        标记是一种对数据包进行一些修改的操作.流量控制中的标记操作会给数据包加上一个DSCP，
        接下来在由一个管理员控制的一个网络下的其他路由器上将会使用这个标记.
```

## Linux流量控制中的组件


``` shell
    流量控制的概念与Linux中流量控制组件的对应关系
        整形(shaping)->class提供流量整形功能
        调度(scheduling)->qdisc就是一个调度器。调度器有比较简单的FIFO，也有比较复杂的如HTB之类的调度器
        分类(classifying)->filter(过滤器)会附着在classifier(分类器)上进行分类工作。 严格来说，分类器必须依靠过滤器才能正常工作.
        策略(policing)->	policer(决策器)可以看成是filter(过滤器)的一个子功能。
        dropping->将filter和policer配合使用，并将policer的动作设为“丢包”，就可以丢弃指定的数据流。
        marking(标记)->dsmark qdisc用于给数据包打上标记。
        
        
    1.qdisc(调度器)
        qdisc其实就是一个调度器。每个网络接口都会有一个调度器，默认的调度器是FIFO.qdiscs会根据调度器的规则重新排列数据包进入队列的顺序
        qdisc是Linux 流量控制系统的核心。qdisc 也被称为排队规则。
        classful qdiscs(分类排队规则)可以包含多个 class(分类)，数据包根据 filter(过滤器)的设置被分发到特定的 classful qdiscs 上。
        分类排队规则可以没有子类，但这样做通常会导致系统资源被白白浪费.
        classless qdiscs 是没有分类的.由于非分类的排队规则没有子对象，所以 classifying(分类)中也没有针对其的操作。
        因此，非分类的排队规则无法和过滤器相关联。
        egress和ingress 存在于每一个网络接口上,在流量控制中最常用的是 egress，也就是我们所熟知的 root qdisc。
        root qdisc 能与任何类型的排队规则相关联，形成流量控制结构。每一个出站的数据包都要经过 egress，
        或者说都要经过 root qdisc 排队规则。网络接口上收到的数据包都将经过 ingress qdisc。ingress qdisc 下无法创建任何分类，
        且只能与一个 filter 相关联。一般情况下，ingress qdisc 仅仅被当成是一个用于附加 policer 来控制入站流量的对象.
        
    2.分类
        类仅存在于可分类的 qdisc之下（如 HTB CBQ 等）理论上，类能无限扩展，一个类可以仅包含一个排队规则，
        也可以包含多个各自排队规则的子类。 一个类之下可以包含多个分类的排队规则.
        
    3.过滤器
        过滤器是Linux流量控制系统中最复杂的对象，它是连接各个流量控制核心组件的纽带。过滤器最简单和最常见的用法就是对数据包进行分类。
        Linux允许用户使用一个或多个过滤器将出站数据包分类并送到不同的出站队列上,一个过滤器必须有一个classifier(分类器)和可以有一个 
        policer(决策器).
        过滤器能与 qdisc 相关联，也可以和 class 相关联。所有的出站数据包首先会通过 root qdisc，
        接着被与root qdisc关联的过滤器进行分类，进入子分类，并被子分类下的过滤器继续进行分类
        
    4 分类器
        过滤器可以使用 tc 命令来进行操作，根据实际需求可以选择使用不同的分类器。最常用的分类器是 u32 分类器，
        它可以根据数据包中的属性对数据包进行分类,分类器可以根据数据包的元数据对数据包进行分类，识别不同类型的数据包。
        在 Linux 流量控制中，分类器的使用是最基本的东西。
        
    5.决策器
        决策器只能配合filter 使用。决策器只有两种操作，当流量高于用户指定值时执行一种操作，反之执行另一种操作.
        policing和shaping都是Linux流量控制中的基本组件，两者都可以对带宽进行限制。但不同的是，shaping 能保存并延迟发送数据包，
        而 policing 只会直接丢弃数据包
        
    6.丢包
        丢包操作只能在 policer 中使用。任何已经与 filter 关联的 policer 都可以使用丢包操作。
        
    7.句柄
        分类器和排队规则句柄编号规则
        
            主编号
                这个编号对内核来说没有意义.用户可以随意指定一个数字。但是，在流量控制结构中，一个对象下所有子对象的主编号必须相同。
                另外，按照惯例，root qdisc 的主编号通常应指定为1。
            
            子编号
                排队规则的子编号必须为0，而分类器的子编号必须是非0值。一个对象下的所有分类器必须有相同的子编号.
```

## 非分类排队规则(非分类 qdisc)

   任何一个非分类排队规则都可以作为网络接口上的根对象，也可以作为分类排队规则的叶子对象。
   这里列出的非分类排队规则是 Linux 下最基本也是最重要的排队规则。
   
``` shell
    1.FIFO, 先入先出
    2.TBF, 令牌桶
        令牌桶排队规则是基于 tokens 和 buckets 这两个概念之上的。要对网络设备上的出站流量进行整形，
        使用令牌桶排队规则是个不错的解决方案，因为它只会减慢数据包发送速率而不会丢弃数据包.
        当有足够的令牌时，数据包才能被发送出去，否则，数据包将被暂存起来延迟发送。
        通过这种方法延迟数据包的发送将会认为地增大数据包的来回时间
        
        (1)创建一个将网络流量速率整形为 256kbit/s 的 TBF
            > tc qdisc add dev eth0 handle 1:0 root dsmark indices 1 default_index 0
            > tc qdisc add dev eth0 handle 2:0 parent 1:0 tbf burst 20480 limit 20480 mtu 1514 rate 256000bps
    
```

## 分类排队规则(分类 qdisc)

分类排队规则赋予了 Linux 流量控制系统极大灵活性。记住，分类排队规则允许与过滤器相关联，并将数据包分配到子分类和子排队规则上去。
与 root qdisc 或父类相关联的分类器有好几种不同的描述方法。与 root qdisc 相关联的分类可以叫做根分类，也可以叫做内部分类。
任何不包含子分类的分类通常被称作叶子分类。在本文中，我们将使用类似树型结构的描述方法，同时也会使用类似家庭成员代词（如孙子节点、兄弟节点）
进行描述。

``` shell
    1.HTB, 分层令牌桶
        HTB 基于令牌和桶的概念，并按照基于对象的方法，同时配合 filter，实现了一个复杂而又精细的流量控制方法。
        利用其提供的 borrowing model 机制，用户能够构造出十分强大而有效的流量控制结构。HTB 最简单，同时也是最直接的使用，
        就是 shaping。
        如果已经充分了解了 tokens 和 buckets 的概念，或者是已经能够熟练使用 TBF，那么掌握 HTB 的使用方法也是水到渠成的事了。
        HTB 允许用户创建一系列具有不同参数的令牌桶，并按需要将这些令牌桶归类，同时配合使用 classifying，就能够实现较细粒度的流量控制。
        
        HTB 最常用的功能就是整形，也就是把出站速率限制在一个固定值之下,所有的整形操作都在叶子分类上进行。
        内部分类和根分类都不会进行整形操作，它们仅在 borrowing model 中用于控制出借令牌应该如何分配
        
        HTB中的节点在不同状态下的不同动作
            节点类型	| 节点状态 |	HTB 内部状态	| 动作
            叶子节点	| < rate  |	HTB_CAN_SEND|	在令牌充足时叶子节点将会发送数据（不超过 burst 个数据包）
            叶子节点	| > rate, < ceil |	HTB_MAY_BORROW |	叶子节点会试图向父节点借用令牌（tokens 或 ctokens）。
                                                            如果父节点有足够的令牌，就会一次性借给子节点 quantum 的整倍数个令牌，
                                                            接着叶子节点发送最多 cburst 个字节。
            叶子节点	| > ceil  |	HTB_CANT_SEND	| 暂缓发送数据包，这将增大数据包延迟，使网络接口上的流量符合整形速率。
            内部节点，根节点 |	< rate	| HTB_CAN_SEND	| 本节点将会出借令牌给子节点。
            内部节点，根节点 |	> rate, < ceil	| HTB_MAY_BORROW |	本节点试图将向父节点借用令牌（tokens 或 ctokens），
                                                                并将借到的令牌按照 quantum 的整数倍出借给子节点。
            内部节点，根节点 |	> ceil	| HTB_CANT_SEND	本节点不会向父节点借用令牌，也不会向子节点出借令牌。
            
         HTB 分类参数
            default:所有 HTB qdisc 对象的默认值，这个参数是可选的。默认的 default 值为0，
                    这意味着所有通过 root qdisc 的数据包将以设备的最大吞吐能力发送出去.
            rate: 设定整形流量速率的最小速度。这个值可以当作承诺信息速率,也可以当作叶子节点的最低带宽
            ceil: 设定整形流量速率的最大速度。租借系统的设置将影响这个参数的实际作用。这个参数可以当作是“突发速率”
            burst: 这是rate令牌桶的大小.HTB 会在令牌（tokens）还没有到来的情况下提前发送 burst 个字节的数据.
            cburst: 这是ceil令牌桶的大小。HTB 会在令牌（ctokens）还没有到来的情况下提前发送 cburst 个字节的数据。
            
         HTB 规则
            (1) 因为在HTB只有叶子节点会对流量进行整形，所以一个分类下的所有叶子节点的 rate 之和不应该超过这个分类的 ceil。
            通常我们建议将分类的 rate 值设定为其所有子节点的 rate 之和，这样，分类随时都能有剩余的带宽（ceil - rate）分配给子节点。
            在 HTB 中只有叶子节点才会对流量进行整形，数据包只会在叶子节点中被暂存，任何中间节点都不会对流量进行整形，
            中间节点只在租借模型中起作用
            
            (2) quantum 仅在当一个节点的速率大于 rate 而小于 ceil 时起作用,quantum的值应该至少设为和MTU 一样大，
                或者比MUT更大。即使quantum 设置值过小，但一有机会 HTB 就会立即发送一个数据包，这将导致 HTB 无法正确计算带宽消耗，
                也就无法正确地对流量进行整形
    
```

## tc 实践

``` shell
    1.流量延时
        > ping cyberciti.biz
        结果:
            64 bytes from 104.20.187.5: icmp_seq=2 ttl=50 time=243 ms
            
        > tc qdisc add dev eth0 root netem delay 200ms  (延时200ms)
        
        > ping cyberciti.biz
        结果:
            64 bytes from 104.20.187.5: icmp_seq=2 ttl=50 time=465 ms
            
    2.要列出当前规则
        > tc -s qdisc ls dev eth0
        
    3.想删除全部规则
        > tc qdisc del dev eth0 root
        
    4.令牌桶过滤器(TBF)
        > tc qdisc add dev ppp0 root tbf rate 220kbit latency 50ms burst 1540
        >tc qdisc add dev eno16780032 root tbf rate 3mbps latency 600s peakrate 4mbps mtu 307200 burst 6144000
        
       参数选项:
            rate表示令牌的产生速率
            limit/latency
                limit确定最多有多少数据（字节数）在队列中等待令牌。你也可以通过设置latency来指定这个参数，
                latency参数确定了一个包在TBF中等待传输的最长等待时间。两者计算决定桶的大小、速率和峰值速率.
                
            burst/buffer/maxburst
                桶的大小，以字节计。这个参数指定了最多可以有多少个令牌能够即刻被使用。通常，管理的带宽越大，需要的缓冲器就越大。
                在Intel体系上，10Mbit/s的速率需要至少10k字节的缓冲区才能达到期望的速率。
                如果你的缓冲区太小，就会导致到达的令牌没有地方放（桶满了），这会导致潜在的丢包
                burst means the maximum amount of bytes that tokens can be available for instantaneously.
                如果数据包的到达速率与令牌的产生速率一致，即200kbit，则数据不会排队，令牌也不会剩余
                如果数据包的到达速率小于令牌的产生速率，则令牌会有一定的剩余。
                如果后续某一会数据包的到达速率超过了令牌的产生速率，则可以一次性的消耗一定量的令牌。
                burst就是用于限制这“一次性”消耗的令牌的数量的，以字节数为单位。
                
            MPU
                一个零长度的包并不是不耗费带宽。比如以太网，数据帧不会小于64字节。
                MPU(Minimum Packet Unit，最小分组单元)决定了令牌的最低消耗
                
            peakrate（峰值速率）
                如果有可用的令牌，数据包一旦到来就会立刻被发送出去，就像光速一样。那可能并不是你希望的，
                特别是你有一个比较大的桶的时候。峰值速率可以用来指定令牌以多快的速度被删除。用书面语言来说，就是：
                释放一个数据包，然后等待足够的时间后再释放下一个。我们通过计算等待时间来控制峰值速率。
                例如：UNIX定时器的分辨率是10毫秒，如果平均包长10kb，我们的峰值速率被限制在了1Mbps
                
            MTU(Maximum Transmission Unit, 最大传输单元)/minburst
                但是如果你的常规速率比较高，1Mbps的峰值速率就需要调整。要实现更高的峰值速率，可以在一个时钟周期内发送多个数据包。
                最有效的办法就是：再创建一个令牌桶！这第二个令牌桶缺省情况下为一个单个的数据包，并非一个真正的桶。
            
                要计算峰值速率，用MTU乘以100就行了。(应该说是乘以HZ数，Intel体系上是100，Alpha体系上是1024)
    
```